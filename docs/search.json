[
  {
    "objectID": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html",
    "href": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "Probability masses of normal and lognormal distributions within the corresponding intervals\n\n\n\n\nThe Jacobian adjustment is a key concept in statistical modeling that arises when transforming probability distributions from one space to another. The intuition is that when you change a random variable, you’re not just changing the values - you’re also changing how “stretched” or “compressed” the probability density becomes at different points. To account for the distortion caused by the transform, the density must be multiplied by a Jacobian adjustment. This ensures that probability masses of corresponding intervals stay unchanged before and after the transform. This is illustrated by the figure above, which compares the probability density functions of a standard normal distribution and its transformed lognormal distribution. Although the shapes of the two distributions differ, the transformation must preserve the probability mass over corresponding intervals.\nIn Bayesian inference, Jacobian adjustments are especially important when transforming parameters from a constrained space (e.g., the positive real line) to an unconstrained space (e.g., the entire real line), which is commonly done to improve sampling efficiency and numerical stability. In Stan, such transformations are typically handled automatically. When you declare a constrained parameter (e.g., &lt;lower=0&gt;), Stan internally transforms it to an unconstrained space and applies the appropriate Jacobian adjustment to maintain the correct posterior density.\nHowever, if you manually transform variables inside the transformed parameters block and assign priors to the transformed variables, you need to explicitly include the Jacobian adjustment to preserve the correct log posterior density (lp__). Failing to do so can lead to biased inference. To illustrate how this works, I’ll use a simple example of normal distribution, focusing on how Stan handles transformations of the standard deviation parameter \\(\\sigma\\) (i.e., &lt;lower=0&gt;) and how we can include a manual Jacobian adjustment.\n\n\nI will first simulate some data from a normal distribution with mean 0 and standard deviation 20. A large standard deviation is chosen to make the effect of Jacobian adjustment more pronounced.\n\ndata_norm &lt;- list(N = 100, y = rnorm(100, 0, 20))\n\n\n\n\nI will include four models to compare posterior parameter estimates and the log posterior density (lp__). The models are:\nModel 1: A normal distribution with a proper constraint on \\(\\sigma\\) Model 2: A normal distribution without a constraint on \\(\\sigma\\) Model 3: A normal distribution with an exponential transformation of unconstrained \\(\\sigma_z\\) and a Jacobian adjustment Model 4: A normal distribution with transformation of \\(\\sigma\\) in the transformed parameters block (No Jacobian needed!)\n\n\nThe first model is a simple normal distribution with a proper constraint on \\(\\sigma\\) (&lt;lower=0&gt;). According to Stan reference manual, to avoid having to deal with constraints while simulating the Hamiltonian dynamics during sampling, every (multivariate) parameter in a Stan model is transformed to an unconstrained variable behind the scenes by the model compiler, and Stan handles the Jacobian adjustment automatically.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real&lt;lower=0&gt; sigma; // standard deviation parameter\n}\n\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm &lt;- stan_model(file = \"./Models/normal.stan\")\nfit_norm &lt;- sampling(md_norm, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nNow we turn to another model by removing the constraint on \\(\\sigma\\). In this case, the parameter \\(\\sigma\\) is not a constrained variable, and there is no Jacobian adjustment handled by Stan. This means that the log posterior density (lp__) is biased.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real sigma; // standard deviation parameter\n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm_no_constraint &lt;- stan_model(file = \"./Models/normal_no_constraint_sigma.stan\")\nfit_norm_no_constraint &lt;- sampling(md_norm_no_constraint, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_no_constraint, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs a comparison, we can also reformulate the model by defining the parameter \\(\\sigma_z\\) as an unconstrained variable, and we then transform it via an exponential function (positive real line). The transformed variable \\(\\sigma\\) will be assigned with a prior and used in the model. This is exactly what has happened internally by Stan when you define a parameter with a proper constraint (e.g., &lt;lower=0&gt; for \\(\\sigma\\)). Stan handles the transformation from an unconstrained internal representation to this constrained user-facing value. Since \\(\\sigma\\) is transformed from \\(\\sigma_z\\), we need to include a Jacobian adjustment to preserve the correct log posterior density (lp__).\n\nLet me explain how the Jacobian adjustment works step by step.\nLet: \\[y = \\sigma_e, \\quad x = \\sigma, \\quad y = \\exp(x)\\]\nWe are transforming from an unconstrained variable \\(x \\in \\mathbb{R}\\) to a positive variable \\(y \\in (0, \\infty)\\).\nNext, we can compute the derivative: \\[\\frac{dy}{dx} = \\frac{d}{dx} \\exp(x) = \\exp(x) = y\\]\nWe apply the change-of-variables formula for densities: \\[\\left|f_Y(y) \\cdot dy\\right| = \\left|f_X(x) \\cdot dx\\right|\n\\quad \\Rightarrow \\quad\nf_Y(y) \\cdot \\left| \\frac{dy}{dx} \\right| = f_X(x)\\]\nSubstituting \\(\\frac{dy}{dx} = y\\), we get: \\[f_Y(y) \\cdot y = f_X(x)\\]\nTaking logs to get log-densities: \\[\\log f_X(x) = \\log f_Y(y) + \\log y\\]\nThis extra term \\(\\log y\\) is the Jacobian adjustment.\nIn Stan notation, we get:\n\\[\\text{target} ~ \\text{+=} ~ \\text{normal\\_lpdf}(\\mu, \\exp(\\sigma_e)) + \\log(\\sigma_e)\\]\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\nparameters {\n  real mu; // mean parameter\n  real sigma_z; // unconstrained standard deviation parameter\n}\ntransformed parameters {\n  real sigma = exp(sigma_z);\n}\nmodel {\n  // Method 1: prior on sigma, with transformed block and Jacobian adjustment\n  target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  // Method 2: local variable sigma, no transformed block, but with Jacobian adjustment\n  // real sigma = exp(sigma_z);\n  // target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma) + log(sigma); // add Jacobian adjustment\n  // target += normal_lpdf(y | mu, sigma) + sigma_z; // alternatively\n}\n\n\nmd_norm_exp_jacobian &lt;- stan_model(file = \"./Models/normal_exp_sigma_jacobian.stan\")\nfit_norm_exp_jacobian &lt;- sampling(md_norm_exp_jacobian, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"lp__\"))\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nIt is also worth mentioning that if you transform the parameter \\(\\sigma\\) in the transformed parameters block without assigning a prior to the transformed parameter, you do not need to include a Jacobian adjustment. This is because the transformation is applied to the parameter after sampling. This is conceptually similar to generating quantities from posterior draws.\nAs a general rule, if you place priors on the declared parameters or directly use the parameters inside parameters block (in most cases), rather than on transformed parameters, no Jacobian adjustment is needed—this is a simple variable transformation. By contrast, if you transform a parameter and place a prior on the transformed variable, you need to include a Jacobian adjustment.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n} \ntransformed parameters {\n  // Method 1: simple transformation without a prior for the transformed parameter\n  real log_sigma = log(sigma); \n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(sigma | 0, 5); // prior on sigma\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_parameters &lt;- stan_model(file = \"./Models/normal_transform_parameters.stan\")\nfit_norm_transform_parameters &lt;- sampling(md_norm_transform_parameters, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_parameters, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nYou may think of it in a different way by transforming the parameter \\(\\sigma\\) via logrithm transformation. This is not what happened under the hood in stan, since Jacobian adjustment is performed on the absolute derivative of the inverse transform. See the stan reference manual for more details.\n\nUnivariate changes of variables Suppose \\(X\\) is one dimensional and \\(f : \\mathrm{supp}(X) \\to \\mathbb{R}\\) is a one-to-one, monotonic function with a differentiable inverse \\(f^{-1}\\). Then the density of \\(Y\\) is given by\n\\[p_Y(y) = p_X(f^{-1}(y)) \\left| \\frac{d}{dy} f^{-1}(y) \\right|\\]\nThe absolute derivative of the inverse transform measures how the scale of the transformed variable changes with respect to the underlying variable.\n\nIf you change in this way, you will change the prior on \\(\\sigma\\). You will not get the same log posterior density (lp__) as Model 1, since the prior on \\(\\sigma\\) is different.\nIn model 1: \\(\\sigma \\sim \\mathcal{N}(0, 5)\\)\nIn model 5: \\(\\log(\\sigma) \\sim \\mathcal{N}(0, 5)\\) or \\(\\sigma \\sim \\mathcal{LogN}(0, 5)\\)\nMy own opinion is that it is not recommended to transform the parameter locally inside the model block, since (1) it is not that transparent unless you really know what you are doing and (2) it will not be saved in the output.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Method 1: prior on log(sigma) --&gt; lead to a different prior on sigma\n  target += normal_lpdf(log(sigma) | 0, 5);\n  target += lognormal_lpdf(sigma | 0, 5); // equivalently\n\n  // Method 2: prior on local variable sigma_log with Jacobian adjustment\n  // real sigma_log = log(sigma);\n  // target += normal_lpdf(sigma_log | 0, 5);\n\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_local &lt;- stan_model(file = \"./Models/normal_transform_local.stan\")\nfit_norm_transform_local &lt;- sampling(md_norm_transform_local, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_local, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs we can see, the posterior parameter estimates for \\(\\mu\\) and \\(\\sigma\\) are similar across all three models. However, the log posterior density (lp__) differs between Model 1 and Model 2. This is because Model 1 includes the proper constraint on \\(\\sigma\\), while Model 2 does not. The log posterior density in Model 2 is biased due to the missing Jacobian adjustment. Model 3 addresses this issue by including a Jacobian adjustment. In general, if you are interested in parameter inference, it may be not a major concern in this case, but missing Jacobian adjustments can cause serious problems for model comparison (e.g., WAIC, LOO, and Bayes factors).\nNote that this example is only for illustration and help you understand the concept of Jacobian adjustment and how Stan handles changes of variables. In practice, you should always use the proper constraint on the parameter and let Stan handle the Jacobian adjustment automatically, which is both more efficient and more reliable.\nRelated links\n\n(Best) A coin toss example with Jacobian transformation\nThe Jacobian transformation\nChanges of variables\nTransforms\nLaplace method and Jacobian of parameter transformation"
  },
  {
    "objectID": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#simulate-data",
    "href": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#simulate-data",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "I will first simulate some data from a normal distribution with mean 0 and standard deviation 20. A large standard deviation is chosen to make the effect of Jacobian adjustment more pronounced.\n\ndata_norm &lt;- list(N = 100, y = rnorm(100, 0, 20))"
  },
  {
    "objectID": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#posterior-parameter-estimates",
    "href": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#posterior-parameter-estimates",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "I will include four models to compare posterior parameter estimates and the log posterior density (lp__). The models are:\nModel 1: A normal distribution with a proper constraint on \\(\\sigma\\) Model 2: A normal distribution without a constraint on \\(\\sigma\\) Model 3: A normal distribution with an exponential transformation of unconstrained \\(\\sigma_z\\) and a Jacobian adjustment Model 4: A normal distribution with transformation of \\(\\sigma\\) in the transformed parameters block (No Jacobian needed!)\n\n\nThe first model is a simple normal distribution with a proper constraint on \\(\\sigma\\) (&lt;lower=0&gt;). According to Stan reference manual, to avoid having to deal with constraints while simulating the Hamiltonian dynamics during sampling, every (multivariate) parameter in a Stan model is transformed to an unconstrained variable behind the scenes by the model compiler, and Stan handles the Jacobian adjustment automatically.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real&lt;lower=0&gt; sigma; // standard deviation parameter\n}\n\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm &lt;- stan_model(file = \"./Models/normal.stan\")\nfit_norm &lt;- sampling(md_norm, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nNow we turn to another model by removing the constraint on \\(\\sigma\\). In this case, the parameter \\(\\sigma\\) is not a constrained variable, and there is no Jacobian adjustment handled by Stan. This means that the log posterior density (lp__) is biased.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real sigma; // standard deviation parameter\n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm_no_constraint &lt;- stan_model(file = \"./Models/normal_no_constraint_sigma.stan\")\nfit_norm_no_constraint &lt;- sampling(md_norm_no_constraint, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_no_constraint, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs a comparison, we can also reformulate the model by defining the parameter \\(\\sigma_z\\) as an unconstrained variable, and we then transform it via an exponential function (positive real line). The transformed variable \\(\\sigma\\) will be assigned with a prior and used in the model. This is exactly what has happened internally by Stan when you define a parameter with a proper constraint (e.g., &lt;lower=0&gt; for \\(\\sigma\\)). Stan handles the transformation from an unconstrained internal representation to this constrained user-facing value. Since \\(\\sigma\\) is transformed from \\(\\sigma_z\\), we need to include a Jacobian adjustment to preserve the correct log posterior density (lp__).\n\nLet me explain how the Jacobian adjustment works step by step.\nLet: \\[y = \\sigma_e, \\quad x = \\sigma, \\quad y = \\exp(x)\\]\nWe are transforming from an unconstrained variable \\(x \\in \\mathbb{R}\\) to a positive variable \\(y \\in (0, \\infty)\\).\nNext, we can compute the derivative: \\[\\frac{dy}{dx} = \\frac{d}{dx} \\exp(x) = \\exp(x) = y\\]\nWe apply the change-of-variables formula for densities: \\[\\left|f_Y(y) \\cdot dy\\right| = \\left|f_X(x) \\cdot dx\\right|\n\\quad \\Rightarrow \\quad\nf_Y(y) \\cdot \\left| \\frac{dy}{dx} \\right| = f_X(x)\\]\nSubstituting \\(\\frac{dy}{dx} = y\\), we get: \\[f_Y(y) \\cdot y = f_X(x)\\]\nTaking logs to get log-densities: \\[\\log f_X(x) = \\log f_Y(y) + \\log y\\]\nThis extra term \\(\\log y\\) is the Jacobian adjustment.\nIn Stan notation, we get:\n\\[\\text{target} ~ \\text{+=} ~ \\text{normal\\_lpdf}(\\mu, \\exp(\\sigma_e)) + \\log(\\sigma_e)\\]\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\nparameters {\n  real mu; // mean parameter\n  real sigma_z; // unconstrained standard deviation parameter\n}\ntransformed parameters {\n  real sigma = exp(sigma_z);\n}\nmodel {\n  // Method 1: prior on sigma, with transformed block and Jacobian adjustment\n  target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  // Method 2: local variable sigma, no transformed block, but with Jacobian adjustment\n  // real sigma = exp(sigma_z);\n  // target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma) + log(sigma); // add Jacobian adjustment\n  // target += normal_lpdf(y | mu, sigma) + sigma_z; // alternatively\n}\n\n\nmd_norm_exp_jacobian &lt;- stan_model(file = \"./Models/normal_exp_sigma_jacobian.stan\")\nfit_norm_exp_jacobian &lt;- sampling(md_norm_exp_jacobian, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"lp__\"))\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nIt is also worth mentioning that if you transform the parameter \\(\\sigma\\) in the transformed parameters block without assigning a prior to the transformed parameter, you do not need to include a Jacobian adjustment. This is because the transformation is applied to the parameter after sampling. This is conceptually similar to generating quantities from posterior draws.\nAs a general rule, if you place priors on the declared parameters or directly use the parameters inside parameters block (in most cases), rather than on transformed parameters, no Jacobian adjustment is needed—this is a simple variable transformation. By contrast, if you transform a parameter and place a prior on the transformed variable, you need to include a Jacobian adjustment.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n} \ntransformed parameters {\n  // Method 1: simple transformation without a prior for the transformed parameter\n  real log_sigma = log(sigma); \n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(sigma | 0, 5); // prior on sigma\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_parameters &lt;- stan_model(file = \"./Models/normal_transform_parameters.stan\")\nfit_norm_transform_parameters &lt;- sampling(md_norm_transform_parameters, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_parameters, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nYou may think of it in a different way by transforming the parameter \\(\\sigma\\) via logrithm transformation. This is not what happened under the hood in stan, since Jacobian adjustment is performed on the absolute derivative of the inverse transform. See the stan reference manual for more details.\n\nUnivariate changes of variables Suppose \\(X\\) is one dimensional and \\(f : \\mathrm{supp}(X) \\to \\mathbb{R}\\) is a one-to-one, monotonic function with a differentiable inverse \\(f^{-1}\\). Then the density of \\(Y\\) is given by\n\\[p_Y(y) = p_X(f^{-1}(y)) \\left| \\frac{d}{dy} f^{-1}(y) \\right|\\]\nThe absolute derivative of the inverse transform measures how the scale of the transformed variable changes with respect to the underlying variable.\n\nIf you change in this way, you will change the prior on \\(\\sigma\\). You will not get the same log posterior density (lp__) as Model 1, since the prior on \\(\\sigma\\) is different.\nIn model 1: \\(\\sigma \\sim \\mathcal{N}(0, 5)\\)\nIn model 5: \\(\\log(\\sigma) \\sim \\mathcal{N}(0, 5)\\) or \\(\\sigma \\sim \\mathcal{LogN}(0, 5)\\)\nMy own opinion is that it is not recommended to transform the parameter locally inside the model block, since (1) it is not that transparent unless you really know what you are doing and (2) it will not be saved in the output.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Method 1: prior on log(sigma) --&gt; lead to a different prior on sigma\n  target += normal_lpdf(log(sigma) | 0, 5);\n  target += lognormal_lpdf(sigma | 0, 5); // equivalently\n\n  // Method 2: prior on local variable sigma_log with Jacobian adjustment\n  // real sigma_log = log(sigma);\n  // target += normal_lpdf(sigma_log | 0, 5);\n\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_local &lt;- stan_model(file = \"./Models/normal_transform_local.stan\")\nfit_norm_transform_local &lt;- sampling(md_norm_transform_local, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_local, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs we can see, the posterior parameter estimates for \\(\\mu\\) and \\(\\sigma\\) are similar across all three models. However, the log posterior density (lp__) differs between Model 1 and Model 2. This is because Model 1 includes the proper constraint on \\(\\sigma\\), while Model 2 does not. The log posterior density in Model 2 is biased due to the missing Jacobian adjustment. Model 3 addresses this issue by including a Jacobian adjustment. In general, if you are interested in parameter inference, it may be not a major concern in this case, but missing Jacobian adjustments can cause serious problems for model comparison (e.g., WAIC, LOO, and Bayes factors).\nNote that this example is only for illustration and help you understand the concept of Jacobian adjustment and how Stan handles changes of variables. In practice, you should always use the proper constraint on the parameter and let Stan handle the Jacobian adjustment automatically, which is both more efficient and more reliable.\nRelated links\n\n(Best) A coin toss example with Jacobian transformation\nThe Jacobian transformation\nChanges of variables\nTransforms\nLaplace method and Jacobian of parameter transformation"
  },
  {
    "objectID": "programming/dplyr-columnwise-rowwise/dplyr-columnwise-rowwise.html",
    "href": "programming/dplyr-columnwise-rowwise/dplyr-columnwise-rowwise.html",
    "title": "Column-wise and Row-wise Operations in dplyr",
    "section": "",
    "text": "row-wise and column-wise operations\n\n\n\n\nWith the development of dplyr or its umbrella package tidyverse, it becomes quite easy to perform operations over columns or rows in R. These column- or row-wise methods can also be directly integrated with other dplyr verbs like select, mutate, filter and summarise, making them more comparable with other functions in apply or map families. In this blog, I will briefly cover some useful column- or row-wise operations.\n\n1 Column-wise operation\nExample 1: select those string columns with less than 5 levels in the dataset of starwars.\n\nstarwars %&gt;%\n  select_if(~ any(is.character(.x) & length(unique(.x)) &lt;= 5)) %&gt;% \n  head()\n\n# A tibble: 6 × 2\n  sex    gender   \n  &lt;chr&gt;  &lt;chr&gt;    \n1 male   masculine\n2 none   masculine\n3 none   masculine\n4 male   masculine\n5 female feminine \n6 male   masculine\n\n\nWe can combine select_if and any to identify certain columns by certain criterion. Note: we are using tilde (~) to define an anonymous function, and thus we should use .x to refer to the selected columns. See this link for detailed illustration of tilde (~), dot (.), and dot x (.x) in dplyr.\nIf you want to calculate the levels of those selected columns, you can try across function and summarise the number of levels by column.\n\nstarwars %&gt;%\n  summarise(across(where(is.character), ~ length(unique(.x))))\n\n# A tibble: 1 × 8\n   name hair_color skin_color eye_color   sex gender homeworld species\n  &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt;   &lt;int&gt;\n1    87         12         31        15     5      3        49      38\n\n\nAlternatively, you can make use of the map or map_dbl function in purrr by the following command. Note that when a map function is applied to a data.frame, it will operate over columns by default.\n\n# map_dbl returns a double vector, while map returns a list\nstarwars %&gt;%\n  select_if(~ is.character(.x)) %&gt;%\n  map_dbl(~length(unique(.x))) %&gt;%\n  head()\n\n      name hair_color skin_color  eye_color        sex     gender \n        87         12         31         15          5          3 \n\n\nExample 2: select those numeric columns and calculate the means and sds across columns in the dataset of starwars.\n\nstarwars %&gt;%\n  summarise(across(where(~ is.numeric(.x)),\n                   list(Mean = ~ mean(.x, na.rm = TRUE),\n                        Sd = ~ sd(.x, na.rm = TRUE))))\n\n# A tibble: 1 × 6\n  height_Mean height_Sd mass_Mean mass_Sd birth_year_Mean birth_year_Sd\n        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1        175.      34.8      97.3    169.            87.6          155.\n\n\nThis example provides us a good illustration of the use of .x in dplyr style syntax, since we have some missing values (NAs) in certain columns. Thus, we need to specify the parameter with na.rm = TRUE inside the functions.\nThere is indeed a more convenient and elegant way of solving this by using the function summarise_if. It allows us to select certain columns and operate by columns like this:\n\nstarwars %&gt;%\n  summarise_if(is.numeric,\n               list(Sum = sum, Mean = mean, Sd = sd),\n               na.rm = TRUE)\n\n# A tibble: 1 × 9\n  height_Sum mass_Sum birth_year_Sum height_Mean mass_Mean birth_year_Mean height_Sd mass_Sd birth_year_Sd\n       &lt;int&gt;    &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1      14143    5741.          3765.        175.      97.3            87.6      34.8    169.          155.\n\n\n\n\n2 Row-wise operation\nExample 3: calculate the sums, means and sds for each row for the dataset of iris.\n\niris %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    Rowsum = sum(c_across(Sepal.Length:Petal.Width)),\n    Rowsd = sd(c_across(Sepal.Length:Petal.Width)),\n    Rowmean = mean(c_across(Sepal.Length:Petal.Width))\n  ) %&gt;%\n  ungroup() %&gt;%\n  head()\n\n# A tibble: 6 × 8\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species Rowsum Rowsd Rowmean\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1          5.1         3.5          1.4         0.2 setosa    10.2  2.18    2.55\n2          4.9         3            1.4         0.2 setosa     9.5  2.04    2.38\n3          4.7         3.2          1.3         0.2 setosa     9.4  2.00    2.35\n4          4.6         3.1          1.5         0.2 setosa     9.4  1.91    2.35\n5          5           3.6          1.4         0.2 setosa    10.2  2.16    2.55\n6          5.4         3.9          1.7         0.4 setosa    11.4  2.23    2.85\n\n\nHere the function c_across is specifically designed to work with rowwise operations. Note: rowwise groups your data by row (class: rowwise_df), and it is best to ungroup immediately. Of course, if you are more comfortable with the apply function, you can also use the following command:\n\niris %&gt;%\n  select(Sepal.Length:Petal.Width) %&gt;%\n  apply(., 1, function(x) c(sum(x), sd(x), mean(x))) %&gt;%\n  as_tibble() %&gt;%\n  t() %&gt;%\n  head()\n\n   [,1]     [,2]  [,3]\nV1 10.2 2.179449 2.550\nV2  9.5 2.036950 2.375\nV3  9.4 1.997498 2.350\nV4  9.4 1.912241 2.350\nV5 10.2 2.156386 2.550\nV6 11.4 2.230844 2.850\n\n\n\niris %&gt;%\n  rowwise() %&gt;%\n  dplyr::mutate(\n    Rowsum = sum(c_across(Sepal.Length:Petal.Width)),\n    Rowmean = mean(c_across(Sepal.Length:Petal.Width)),\n    Rowsd = sd(c_across(Sepal.Length:Petal.Width)),\n    .before = \"Species\"\n  ) %&gt;%\n  ungroup() %&gt;%\n  head()\n\n# A tibble: 6 × 8\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Rowsum Rowmean Rowsd Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2   10.2    2.55  2.18 setosa \n2          4.9         3            1.4         0.2    9.5    2.38  2.04 setosa \n3          4.7         3.2          1.3         0.2    9.4    2.35  2.00 setosa \n4          4.6         3.1          1.5         0.2    9.4    2.35  1.91 setosa \n5          5           3.6          1.4         0.2   10.2    2.55  2.16 setosa \n6          5.4         3.9          1.7         0.4   11.4    2.85  2.23 setosa \n\n\n\niris %&gt;%\n  as_tibble() %&gt;%\n  dplyr::mutate(\n    row = pmap(across(1:4), ~ {\n      list(rsum = sum, rmean = mean) %&gt;%\n        map_dfc(function(f) f(c(...)))\n    }),\n    .before = \"Sepal.Length\"\n  ) %&gt;%\n  unnest(row) %&gt;%\n  head()\n\n# A tibble: 6 × 7\n   rsum rmean Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1  10.2  2.55          5.1         3.5          1.4         0.2 setosa \n2   9.5  2.38          4.9         3            1.4         0.2 setosa \n3   9.4  2.35          4.7         3.2          1.3         0.2 setosa \n4   9.4  2.35          4.6         3.1          1.5         0.2 setosa \n5  10.2  2.55          5           3.6          1.4         0.2 setosa \n6  11.4  2.85          5.4         3.9          1.7         0.4 setosa \n\n\n\niris %&gt;%\n  as_tibble() %&gt;%\n  dplyr::mutate(\n    row = pmap(\n      across(Sepal.Length:Petal.Width),\n      ~ bind_cols(\n        rsum = sum(c(...)),\n        rmean = mean(c(...)),\n        rsd = sd(c(...))\n      )\n    ),\n    .before = \"Sepal.Length\"\n  ) %&gt;%\n  unnest(row) %&gt;%\n  head()\n\n# A tibble: 6 × 8\n   rsum rmean   rsd Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1  10.2  2.55  2.18          5.1         3.5          1.4         0.2 setosa \n2   9.5  2.38  2.04          4.9         3            1.4         0.2 setosa \n3   9.4  2.35  2.00          4.7         3.2          1.3         0.2 setosa \n4   9.4  2.35  1.91          4.6         3.1          1.5         0.2 setosa \n5  10.2  2.55  2.16          5           3.6          1.4         0.2 setosa \n6  11.4  2.85  2.23          5.4         3.9          1.7         0.4 setosa \n\n\n\n\n\n3 Related links:\n\nhttps://dplyr.tidyverse.org/articles/rowwise.html\nhttps://purrr.tidyverse.org/reference/map.html"
  },
  {
    "objectID": "programming/intro-purrr/intro-purrr.html",
    "href": "programming/intro-purrr/intro-purrr.html",
    "title": "A Gentle Introduction to purrr",
    "section": "",
    "text": "1 Introduction\nThe purrr package in R provides a powerful set of tools for working with lists and vectors in a functional programming style. Functions like map(), map_lgl(), map_chr(), map_int(), and map_dbl() allow you to iterate over elements cleanly and efficiently—offering a more readable and pipe-friendly alternative to for loops and lapply().\nFor example, here is a comparison of using a for loop versus purrr::map_dbl():\n\n\n\n\n\nComparison between for loops and map dbl functions\n\n\n\n\nThe map family contains a number of type-specific variants. While map() returns a list and supports varying return types and lengths, its variants ensure a consistent output format:\n\n\n\nFunction\nOutput Type\n\n\n\n\nmap()\nList\n\n\nmap_dbl()\nDouble / numeric vector\n\n\nmap_int()\nInteger vector\n\n\nmap_lgl()\nLogical vector\n\n\nmap_chr()\nCharacter vector\n\n\nmap_dfr()\nData frame (row bind)\n\n\nmap_dfc()\nData frame (col bind)\n\n\n\n\n\n2 Select Elements\n\nlist_abc &lt;- list(a = c(1, 2), b = c(3, 4, 5), c = c(\"m\", \"n\"))\n\nExample 1: Select elements by name or index\n\n# Recommended: using subset\nlist_abc %&gt;% .[c(\"a\", \"b\")]  # or .[1:2]\n\n# Using magrittr::extract\nlist_abc %&gt;% magrittr::extract(c(\"a\", \"c\"))\n\n# Base R style\nlist_abc[c(\"a\", \"b\")]\n\n$a\n[1] 1 2\n\n$b\n[1] 3 4 5\n\n$a\n[1] 1 2\n\n$c\n[1] \"m\" \"n\"\n\n$a\n[1] 1 2\n\n$b\n[1] 3 4 5\n\n\n\n\n3 Filter Elements\nExample 2: Filter based on conditions\n\n# Keep elements with length 2\nlist_abc %&gt;% keep(~ length(.x) == 2)\n\n# Discard character vectors\nlist_abc %&gt;% discard(is.character)\n\n# Drop NULL elements\nlist_abc %&gt;% append(list(d = NULL)) %&gt;% compact()\n\n$a\n[1] 1 2\n\n$c\n[1] \"m\" \"n\"\n\n$a\n[1] 1 2\n\n$b\n[1] 3 4 5\n\n$a\n[1] 1 2\n\n$b\n[1] 3 4 5\n\n$c\n[1] \"m\" \"n\"\n\n\nExample 3: Slice elements inside the list\n\n# First element from each\nlist_abc %&gt;% map(1)\n\n# First two elements from each\nlist_abc %&gt;% map(~ .x[1:2])\n\n$a\n[1] 1\n\n$b\n[1] 3\n\n$c\n[1] \"m\"\n\n$a\n[1] 1 2\n\n$b\n[1] 3 4\n\n$c\n[1] \"m\" \"n\"\n\n\n\n\n4 Modify Elements\nExample 4: Modify with conditions\n\n# Add 1 to numeric elements\nlist_abc %&gt;% keep(is.numeric) %&gt;% modify(~ .x + 1)\n\n# Modify if numeric, leave others unchanged\nlist_abc %&gt;% modify_if(is.numeric, ~ .x + 1)\n\n# Modify elements at positions 1 and 2\nlist_abc %&gt;% modify_at(1:2, ~ .x + 10)\n\n$a\n[1] 2 3\n\n$b\n[1] 4 5 6\n\n$a\n[1] 2 3\n\n$b\n[1] 4 5 6\n\n$c\n[1] \"m\" \"n\"\n\n$a\n[1] 11 12\n\n$b\n[1] 13 14 15\n\n$c\n[1] \"m\" \"n\"\n\n\n\n\n5 Combine Lists\nExample 5: Combine multiple lists\n\na &lt;- list(a = 1:2)\nb &lt;- list(b = 3:4)\nc &lt;- list(c = 5:6)\n\n# Append b to a\na %&gt;% append(b)\n\n# Prepend b to a\na %&gt;% prepend(b)\n\n# Splice multiple lists together\na %&gt;%\n  splice(b, c) %&gt;%\n  set_names(c(\"A\", \"B\", \"C\"))  # or use: set_names(toupper)\n\n$a\n[1] 1 2\n\n$b\n[1] 3 4\n\n$b\n[1] 3 4\n\n$a\n[1] 1 2\n\n$A\n[1] 1 2\n\n$B\n[1] 3 4\n\n$C\n[1] 5 6\n\n\n\n\n6 Summarize Elements\nExample 6: Reduce to a single result\n\nlist_abc &lt;- list(a = 1:2, b = 3:4, c = 5:6)\n\n# Element-wise sum\nlist_abc %&gt;% reduce(`+`)\n\n# Element-wise multiplication\nlist_abc %&gt;% reduce(`*`)\n\n[1]  9 12\n[1] 15 48\n\n\nExample 7: Reduce by groups\n\nlist_abc &lt;- list(a = 1:2, b = 3:4, c = 5:6)\n\n# Group-wise summation: first two together, third separately\nlist(1:2, 3) %&gt;% map(~ reduce(list_abc[.x], `+`))\n\n[[1]]\n[1] 4 6\n\n[[2]]\n[1] 5 6\n\n\n\n\n7 Further Reading & Resources\n\npurrr Cheat Sheet (RStudio)\nUsing the purrr Package (r4epi)\npurrr extras — Stanford DCL\npurrr for Parallelism — Stanford DCL"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Data Science Toolkit",
    "section": "",
    "text": "Data Science Toolkit aims to provide comprehensive, practical resources for learners and practitioners. I believe that the right tools and knowledge should be accessible to everyone interested in learning to become a data scientist."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About Data Science Toolkit",
    "section": "",
    "text": "Data Science Toolkit aims to provide comprehensive, practical resources for learners and practitioners. I believe that the right tools and knowledge should be accessible to everyone interested in learning to become a data scientist."
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "",
    "text": "Why I Switched from Kitty and Vifm to Ghostty and Yazi? After years of experimenting with different terminal emulators and file managers, I’ve finally found my favorite combination: Ghostty and Yazi. These have recently gained a lot of popularity in the community. Briefly put, Ghostty is a modern terminal emulator that is fast, feature-rich, and native. Yazi is a blazing-fast terminal file manager. Together, they have transformed my daily workflow. In this blog, I’ll show you how to set them up on your own computer, so you can supercharge your terminal experience just like I did.\nNote: here I assume the default fish shell is used. You can install it in this blog."
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-ghostty",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-ghostty",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "2.1 brew install ghostty",
    "text": "2.1 brew install ghostty\nbrew install --cask ghostty"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-ghostty-as-default-terminal-emulator-on-mac",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-ghostty-as-default-terminal-emulator-on-mac",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "2.2 set ghostty as default terminal emulator on mac",
    "text": "2.2 set ghostty as default terminal emulator on mac\n\n\nInstall the “RCDefaultApp.prefPane” plugin\n\n\nThis plugin is used to set the default app for opening terminal in your system.\ngit clone https://github.com/JakeJing/fishconfig.git\nsudo mv fishconfig/kitty/RCDefaultApp.prefPane /Library/PreferencePanes/\n\n\nSet ghostty as the “default app” for opening terminal\n\n\nGo to system preferences -&gt; default Apps -&gt; click the “default Apps” -&gt; URLS -&gt; x-man-page -&gt; set the default application as “ghostty”.\n\n\n\nSet ghostty as default app for opening terminal\n\n\n\n\nAdd keyboard shortcut (shift-cmd-1) to open a new Ghostty window here\n\n\nYou can set a keyboard shortcut to open Ghostty here. However, the default option only works when your cursor is on a folder. To enable it when your cursor is on a file, you can create a new service using the Automator application. To do this, go to Automator -&gt; Quick Action, and follow the steps in the image below.\n\n\n\nAutomator Quick Action\n\n\nAfter that, you can go to System Preferences -&gt; Keyboard -&gt; Shortcuts -&gt; Services -&gt; open-ghostty-here to add it (shift-cmd-1).\n\n\n\n\nAdd keyboard shortcut to open Ghostty here\n\n\nThis will automatically open a new Ghostty window here when you press shift-cmd-1. If there is an ongoing Ghostty window, it will open a new tab instead. So far there is no easy way to always open a new window (rather than a new tab or a new process) on mac, as far as I know.\n# check the services\nls ~/Library/Services/\n\n\nadd configuration file for ghostty\n\n\nwget https://raw.githubusercontent.com/JakeJing/dotfiles/refs/heads/main/.config/ghostty/config -P ~/.config/ghostty/"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-yazi-and-its-dependencies",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-yazi-and-its-dependencies",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "3.1 brew install yazi and its dependencies",
    "text": "3.1 brew install yazi and its dependencies\nbrew update\nbrew install yazi ffmpeg sevenzip jq poppler fd ripgrep fzf zoxide resvg imagemagick font-symbols-only-nerd-font"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-alias-in-fish-config",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-alias-in-fish-config",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "3.2 set alias in fish config",
    "text": "3.2 set alias in fish config\n# yazi\nalias yz yazi\nalias y yazi\nalias a yazi"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#add-configuration-file-and-keymap-for-yazi",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#add-configuration-file-and-keymap-for-yazi",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "3.3 add configuration file and keymap for yazi",
    "text": "3.3 add configuration file and keymap for yazi\n# clone my dotfiles\ngit clone https://github.com/JakeJing/dotfiles.git\n# move yazi config\nmv dotfiles/.config/yazi -P ~/.config/yazi/\n\n\n\nUseful Yazi Keybindings"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "",
    "text": "Clone the dotfiles and copy the Neovim config into your ~/.config directory:\n\ngit clone https://github.com/JakeJing/dotfiles.git\nmv dotfiles/.config/nvim -P ~/.config/\n\nOpen plugins.lua and type :w to launch the auto-installation.\nUse :checkhealth to verify that all dependencies are properly set up."
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#installation",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#installation",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "",
    "text": "Clone the dotfiles and copy the Neovim config into your ~/.config directory:\n\ngit clone https://github.com/JakeJing/dotfiles.git\nmv dotfiles/.config/nvim -P ~/.config/\n\nOpen plugins.lua and type :w to launch the auto-installation.\nUse :checkhealth to verify that all dependencies are properly set up."
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#navigation-movements",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#navigation-movements",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "2 Navigation & Movements",
    "text": "2 Navigation & Movements\n\nhjkl: move left, down, up, right\nCtrl+d/u: scroll down/up one page\nCtrl+n/p: next/previous file in window\ni/I: insert before cursor / at line start\na/A: append after cursor / at line end\n0 / ^: go to line start\n$: go to line end\no/O: new line below/above\n%: jump between matching brackets\nb/e/w: word navigation (back, end, next)\nf{x} / F{x}: move to char {x} forward/back\nt{x} / T{x}: move before char {x} forward/back\n'' or g:: go back to previous position\no (in visual mode): toggle selection endpoint"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#bookmarks-with-telescope",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#bookmarks-with-telescope",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "3 Bookmarks (with Telescope)",
    "text": "3 Bookmarks (with Telescope)\n\nmm: toggle bookmark\nShift+n: next bookmark\nShift+b: previous bookmark\nma: view all bookmarks\n:Telescope vim_bookmarks all: list all bookmarks\n:Telescope vim_bookmarks current_file: list for current file"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#deleting-text",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#deleting-text",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "4 Deleting Text",
    "text": "4 Deleting Text\n\ndd: delete line\n3dd: delete 3 lines\nD or d$: delete to line end\nd0: delete to line start\nx: delete character under cursor\ndw, diw, daw: delete word (various scopes)\ndip: delete paragraph\ndi\": delete inside quotes\n:%d: delete entire file content\nci\" / ci(: delete and insert inside quotes/brackets"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#joining-lines",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#joining-lines",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "5 Joining Lines",
    "text": "5 Joining Lines\n\nJ: join lines with space\ngJ: join lines without space"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#increment-numbers",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#increment-numbers",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "6 Increment Numbers",
    "text": "6 Increment Numbers\n\nSelect numbers with Ctrl+v, then g Ctrl+a to increment"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#word-count",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#word-count",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "7 Word Count",
    "text": "7 Word Count\n\n:WordCount: custom word count (requires user-defined function)\n&lt;leader&gt;wc: shortcut for word count"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#selection-visual-mode",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#selection-visual-mode",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "8 Selection (Visual Mode)",
    "text": "8 Selection (Visual Mode)\n\nv, V, Ctrl+v: character, line, and block selection\nviw, vaw: inside/around word\nvi{: inside block (like function)\nf{char} in visual mode: fast selection"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#multi-line-insert-append",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#multi-line-insert-append",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "9 Multi-line Insert & Append",
    "text": "9 Multi-line Insert & Append\n\nNote: Only works in visual block mode (Ctrl+v).\n\n\nInsert start: Ctrl+v, select, Shift+i, type, then Esc\nAppend end: Ctrl+v, select, Shift+a, type, then Esc\nChange text: select block, press c, type, then Esc"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#editing-text",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#editing-text",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "10 Editing Text",
    "text": "10 Editing Text\n\nci(: change inside parentheses\ncip: change inside paragraph\ncw: change word"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#yank-paste",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#yank-paste",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "11 Yank & Paste",
    "text": "11 Yank & Paste\n\nyy, yiw, ya(, y2w: yank lines, words, brackets\nUse neoclip for yank history:\n\n&lt;C-c&gt; to yank\n&lt;C-p&gt; to paste from history"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#substitution",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#substitution",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "12 Substitution",
    "text": "12 Substitution\n\nS: start substitution (:%s//g via remap)\ncgn: change next match, repeat with .\nRegex substitution:\n:%s/[-.+/a-zA-Z0-9\"$]*\\ze:/`\\0`/g\nR: overwrite text\n:'&lt;,'&gt;s/old/new/g: substitute in selection\n:cdo s/old/new/g: substitute across quickfix list"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#repeating-undoing",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#repeating-undoing",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "13 Repeating & Undoing",
    "text": "13 Repeating & Undoing\n\n.: repeat last change\nu / U: undo / undo line\nCtrl+r: redo\nCtrl+s: open terminal in vertical split\n:vs | :term: open vertical terminal"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#quit-save",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#quit-save",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "14 Quit & Save",
    "text": "14 Quit & Save\n\nZZ: save and quit\nZQ: quit without saving"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#key-mapping-help",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#key-mapping-help",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "15 Key Mapping & Help",
    "text": "15 Key Mapping & Help\n\n15.1 Special Keys\n&lt;Tab&gt;, &lt;CR&gt;, &lt;Esc&gt;, &lt;Space&gt;, &lt;A-j&gt;, &lt;C-s&gt;, &lt;Up&gt;, &lt;F1&gt;...&lt;F12&gt;, etc.\n\n\n15.2 Check Mappings\n:imap &lt;A-j&gt;\n:verbose imap &lt;Tab&gt;\n\n\n15.3 Help\n\n:h {key} or :help ctrl-w_&lt;\n:h telescope.command, :h regex\nfh or &lt;leader&gt;fh: floating help"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#terminal-tricks",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#terminal-tricks",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "16 Terminal Tricks",
    "text": "16 Terminal Tricks\n\nCtrl+s: toggle floating terminal\n:r !ls: insert output of shell command"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#termvifm-zoxide",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#termvifm-zoxide",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "17 TermVifm + Zoxide",
    "text": "17 TermVifm + Zoxide\n\nBind vf to launch vifm\nUse zoxide with &lt;leader&gt;Z to jump to dirs\n\nInstall with brew install zoxide\nAdd to config.fish:\nzoxide init fish | source"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#search",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#search",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "18 Search",
    "text": "18 Search\n\nff: find file\nfa: find all in buffer\nfw: find word\n'Search: exact match\n:set hlsearch / :nohlsearch: toggle highlights"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#buffers",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#buffers",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "19 Buffers",
    "text": "19 Buffers\n\n:bfirst, :blast, :bnext, :bprevious\n:ls: list buffers\n:bd [num]: close buffer\n:b &lt;TAB&gt;: autocomplete open buffers\nCtrl+^: toggle last buffer"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#window-management",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#window-management",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "20 Window Management",
    "text": "20 Window Management\n\nCtrl+w w: switch windows\nCtrl+w h/l: left/right\nCtrl+w _: maximize\nCtrl+w =: equalize\nCtrl+w R: reverse splits\nCtrl+w t Ctrl+w K: horizontal → vertical\nCtrl+w t Ctrl+w H: vertical → horizontal\nCtrl+Cmd+F or fn+F: full screen"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#git-with-lazygit",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#git-with-lazygit",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "21 Git with LazyGit",
    "text": "21 Git with LazyGit\n\nCtrl+g: open LazyGit\nc: commit\nShift+p: push\n?: help\ngl: git logs"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#linting-formatting",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#linting-formatting",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "22 Linting & Formatting",
    "text": "22 Linting & Formatting\n\n:NullLsInfo: see current null-ls status\n:echo executable(\"eslint\"): check if installed\n:LspStop: stop diagnostics"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#registers-special-characters",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#registers-special-characters",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "23 Registers & Special Characters",
    "text": "23 Registers & Special Characters\n\"%\": current file\n\"#\": alternate file\n\"*\", \"+\": system clipboard\n\"/\": last search\n\":\": last command\n\"-\": last small delete\n\".\": last insert\n\"=\": expression register"
  },
  {
    "objectID": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#cmdline-modes",
    "href": "commandline/nvim-cheatsheet/nvim-cheatsheet.html#cmdline-modes",
    "title": "Neovim Cheat Sheet: Essential Tricks and Shortcuts",
    "section": "24 Cmdline Modes",
    "text": "24 Cmdline Modes\n:   Normal command\n&gt;   Debug mode\n/   Forward search\n?   Backward search\n=   Expression\n@   Input()\n-   Insert/append text\n\nq:: show command history"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Data Science Toolkit",
    "section": "",
    "text": "Curated collections of programming languages for specific data science tasks:\n\n🧪 R Ecosystem: Tidyverse, and statistical packages\n📈 Stan: Bayesian analysis, Probabilistic programming\n🎨 Data Visualization: ggplot2, Plotly\n\n\n\n\nStep-by-step guides and hands-on examples:\n\n🧮 Ghostty / Kitty\n🗂️ Yazi / Vifm\nHelix\n📊 Visidata\nNvim\n\n\n\n\nAdditional learning materials:\n\n📖 Recommended Books and Courses\n📂 Datasets for Practice\n🌐 Community Resources"
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Welcome to Data Science Toolkit",
    "section": "",
    "text": "Curated collections of programming languages for specific data science tasks:\n\n🧪 R Ecosystem: Tidyverse, and statistical packages\n📈 Stan: Bayesian analysis, Probabilistic programming\n🎨 Data Visualization: ggplot2, Plotly\n\n\n\n\nStep-by-step guides and hands-on examples:\n\n🧮 Ghostty / Kitty\n🗂️ Yazi / Vifm\nHelix\n📊 Visidata\nNvim\n\n\n\n\nAdditional learning materials:\n\n📖 Recommended Books and Courses\n📂 Datasets for Practice\n🌐 Community Resources"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "",
    "text": "pip3 install visidata\nvd --version"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#adjusting-columns",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#adjusting-columns",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.1 Adjusting Columns",
    "text": "5.1 Adjusting Columns\n\nHide a column: -\nShow all hidden columns: gv\nAuto-adjust column width: _"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#adding-columns-and-rows",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#adding-columns-and-rows",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.2 Adding Columns and Rows",
    "text": "5.2 Adding Columns and Rows\n\nza: Append a blank column\ni: Insert new column\n^: Rename column\na: Add a new row (not editable in frequency table)\nDelete a column: Shift+C, then d\nEdit a cell: e"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#navigation",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#navigation",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.3 Navigation",
    "text": "5.3 Navigation\n\nGo to beginning of column: gh\nGo to end of column: gl"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#overview-summary",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#overview-summary",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.4 Overview / Summary",
    "text": "5.4 Overview / Summary\n\nShift+I: Overview (“bird’s eye view”) of the data"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#selecting-and-deselecting-rows",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#selecting-and-deselecting-rows",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.5 Selecting and Deselecting Rows",
    "text": "5.5 Selecting and Deselecting Rows\n\n,: Select rows matching the current cell in the current column\ns: Select row\ngs: Select all\nu: Deselect current row\ngu: Deselect all\ngd: Delete all selected rows\nSelect by pattern:\nUse / to search column\nUse g/ to search all columns\nUse regex with \\... to deselect (e.g., rows longer than 3 chars)\nFrom frequency table:\n\nSelect rows (s), press g+Enter to go back\nUse \" to copy\nAlternative: Shift+F → select rows → q to go back"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#filtering",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#filtering",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.6 Filtering",
    "text": "5.6 Filtering\n\nz|: Filter rows using expressions\nExamples:\n\nOPERATOR == \"BUSINESS\"\nSTATE != \"FL\"\nHeight &gt; 170"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#deleting",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#deleting",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.7 Deleting",
    "text": "5.7 Deleting\n\nDelete row: d\nDelete column: Shift+C → d\nDelete all selections: gd"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#replace-values",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#replace-values",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.8 Replace Values",
    "text": "5.8 Replace Values\n\nSelect matching rows with ,\nge: Globally edit selected cells in a column"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#sorting",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#sorting",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.9 Sorting",
    "text": "5.9 Sorting\n\nSort ascending: [\nSort descending: ]"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#plotting",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#plotting",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.10 Plotting",
    "text": "5.10 Plotting\n\nSelect x-axis column: !\nSelect y-axis column: !\nOptional: select color column\nEnsure cursor is on y-axis (numeric)\nPlot: .\nAdjust aspect ratio: z_ (plot width / height)\nConvert to numeric: % or #"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#command-prompt",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#command-prompt",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.11 Command Prompt",
    "text": "5.11 Command Prompt\n\nSpace: Launch command prompt"
  },
  {
    "objectID": "commandline/visidata-commands-tricks/visidata-commands.html#mnemonics",
    "href": "commandline/visidata-commands-tricks/visidata-commands.html#mnemonics",
    "title": "VisiData: Commands, Tips, and Tricks",
    "section": "5.12 Mnemonics",
    "text": "5.12 Mnemonics\n\ng = “global” or “all”:\n/ = search in column\ng/ = search in all columns\nz = “zoom in” or “narrow”:\ny = copy column\nzy = copy cell"
  },
  {
    "objectID": "commandline/essential-helix-commands/essential-helix.html",
    "href": "commandline/essential-helix-commands/essential-helix.html",
    "title": "Essential Helix Commands",
    "section": "",
    "text": "Note: In Helix, Alt (or A) functions similarly to Cmd in Vim. Importantly, Helix uses a selection-first model—you must select text before performing most actions.\n\n1 Navigation\n\nh / j / k / l: Move left, down, up, right\nw / W / e / E / b / B: Word-based motion\ni / a / I / A: Insert modes at various positions\nf / F / t / T: Jump to characters in the current line\ngg: Go to beginning of file\nge: Go to end of file\ngh: Go to beginning of line\ngl: Go to end of line\nu / U: Undo / redo\ngr: Go to reference\ngd: Go to definition\n\n\n\n2 Deletion & Change\n\nc: Change character or selection\nxd or dd: Delete line (remapped)\n:%d: Delete entire file\n\n\n\n3 File & Command Access\n\n&lt;Space&gt; f: File picker\n&lt;Space&gt; b: Buffer picker (all files in buffer)\n&lt;Space&gt; ?: Command palette\nZZ: Save and close\nZQ: Quit without saving\n:q: Quit\n\n\n\n4 Window Management\n\nCtrl-w v: Split window vertically\nzz: Center the current line on screen\n\n\n\n5 Selection\n\nv 2w: Select the next two words\nx: Expand selection downward\nX: Expand selection upward\n5x: Select current + 4 lines below\n2xv: Combine with jk to resize selection\nmi\": Select inside quotes\nmiw: Select inside word\nmip: Select inside paragraph\n%: Select entire file\n%s&lt;pattern&gt;: Select file and match pattern (regex supported); press Esc then , to exit multiple cursors\n&lt;Space&gt; /: Grep for word in current directory, then Ctrl+v to open matches in vertical split\n\n\n\n6 Multiple Cursors\n\nC: Duplicate cursor to the next match/line\n,: Remove the most recent cursor\nvj C C C: Select 2 lines and press C repeatedly to create multiple cursors every 2 lines\nCtrl+A / Ctrl+E: Move all cursors to start/end of line\nI / A: Enter insert mode across all cursors\n\n\n\n7 Copy & Paste\n\nx: Select line\np: Paste from register\n&lt;Space&gt; y: Copy selection to global register\n\n\n\n8 Commands\n\n| or Cmd+!: Pipe selection through shell command (e.g., | sort to sort lines)\n\n\n\n9 Definitions & Diagnostics\n\ngd: Go to definition\nCtrl+o: Go back to previous location\nCtrl+i: Go forward\n&lt;Space&gt; k: Show function help"
  },
  {
    "objectID": "commandline/coverpage.html",
    "href": "commandline/coverpage.html",
    "title": "Command Line Tools",
    "section": "",
    "text": "Transform Your Terminal with Ghostty and Yazi\n\n\nEssential Helix Commands\n\n\nvisidata commands and tricks\n\n\nnvim cheatsheet"
  },
  {
    "objectID": "programming/nested_splitted_data_structure/nested_splitted_structure.html",
    "href": "programming/nested_splitted_data_structure/nested_splitted_structure.html",
    "title": "Nested vs. Splitted Structures in Data Simulation and Inference",
    "section": "",
    "text": "nest and split operations for a data.frame\nThe development of dplyr and purrr packages makes the workflow of R programming more smooth and flexible. The dplyr package provides an elegant way of manipulating data.frames or tibbles in a column-wise (e.g., select, filter, mutate, arrange, group_by, summarise and case_when) or row-wise (rowwise, c_across, and ungroup) manner. It also fits well with the map function by applying an anonymous function to each column, or applying a user-defined function to each row.\nThe purrr package allows us to map a function to each element of a list. You can also select, filter, modify, combine and summarise a list (see this blog for an overview). Note that the default output from map function is a list of the same length as the input data, though you can easily reformat the output into a data.frame via map_dfr and map_dfc functions.\nHere we focus on comparing and understanding the nest and split functions in data simulation and inference. Specifically, two data structures (nested data vs. splitted data) are used for simulating and fitting linear regression models across different types."
  },
  {
    "objectID": "programming/nested_splitted_data_structure/nested_splitted_structure.html#function",
    "href": "programming/nested_splitted_data_structure/nested_splitted_structure.html#function",
    "title": "Nested vs. Splitted Structures in Data Simulation and Inference",
    "section": "1.1 Function",
    "text": "1.1 Function\nWe first define a function to generate the response (y) by providing the predictor (x), intercept and slope.\n\n# function to generate the response\ngenerate_response &lt;- function(x, intercept, slope) {\n  x * slope + intercept + rnorm(length(x), 0, 30)\n}"
  },
  {
    "objectID": "programming/nested_splitted_data_structure/nested_splitted_structure.html#simulation-with-given-parameters",
    "href": "programming/nested_splitted_data_structure/nested_splitted_structure.html#simulation-with-given-parameters",
    "title": "Nested vs. Splitted Structures in Data Simulation and Inference",
    "section": "1.2 Simulation with given parameters",
    "text": "1.2 Simulation with given parameters\nTo simulate data for each type (A, B or C), we put the parameters in a tibble, since it allows nested objects with a list of vectors as a column. To generate the response, we use a rowwise function by applying the generate_response function to each row.\n\n# it is recommended to use tibble format\nparameters &lt;- tibble(\n  type = c(\"A\", \"B\", \"C\"),\n  x = list(1:100, 1:100, 1:100),\n  intercept = c(1, 3, 5),\n  slope = c(2, 4, 3)\n)\n# note: convert the vector responses into a list\nsimulated_df &lt;- parameters %&gt;%\n  rowwise() %&gt;%\n  mutate(y = list(generate_response(x, intercept, slope))) %&gt;%\n  ungroup() %&gt;%\n  unnest(c(x, y))\n\n\n\n\nSimulated data plot"
  },
  {
    "objectID": "programming/nested_splitted_data_structure/nested_splitted_structure.html#run-the-linear-model",
    "href": "programming/nested_splitted_data_structure/nested_splitted_structure.html#run-the-linear-model",
    "title": "Nested vs. Splitted Structures in Data Simulation and Inference",
    "section": "1.3 Run the linear model",
    "text": "1.3 Run the linear model\nWith the simulated data.frame or tibble, we can create a nested data and map the linear model for each type. After that, we can extract the predicted values and 95% credible intervals.\n\n# nesting data by each type and run lm via map\nlm_results &lt;- simulated_df %&gt;%\n  group_by(type) %&gt;%\n  nest() %&gt;%\n  mutate(\n    models = map(data, ~ lm(y ~ x, data = .x)),\n    summaries = map(models, ~ broom::glance(.x)),\n    model_coef = map(models, ~ broom::tidy(.x)),\n    pred = map(models, ~ predict(.x, interval = \"confidence\"))\n  )\n# extract the predicted results\npred_ci &lt;- lm_results %&gt;%\n  dplyr::select(type, pred) %&gt;%\n  unnest(pred) %&gt;%\n  pull(pred) %&gt;%\n  set_colnames(c(\"fit\", \"lwr\", \"upr\"))\n\n\n\n\nLinear model results"
  },
  {
    "objectID": "programming/nested_splitted_data_structure/nested_splitted_structure.html#visualization",
    "href": "programming/nested_splitted_data_structure/nested_splitted_structure.html#visualization",
    "title": "Nested vs. Splitted Structures in Data Simulation and Inference",
    "section": "1.4 Visualization",
    "text": "1.4 Visualization\nTo visualize the raw data and the fitted lines, we need to combine them by row, and draw the fitted line and credible intervals via geom_line and geom_ribbon functions from ggplot2.\n\ncbind(simulated_df, pred_ci) %&gt;%\n  ggplot(., aes(x = x, y = y, color = type)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = lwr, ymax = upr, fill = type, color = NULL),\n    alpha = .6\n  ) +\n  geom_line(aes(y = fit), size = 1) +\n  facet_wrap(~type) +\n  theme(legend.position = \"none\")\n\n\n\n\nFitted results from the linear regression models"
  },
  {
    "objectID": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html",
    "href": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "",
    "text": "In this blog, I will provide a step-by-step instruction of how we can generate data from a mixed effect model and recover the parameters of the model with the simulated dataset. This simulation-based experiment can help us better understand the structure and generative process of the multilevel model with correlated random intercepts and slopes. To proceed, I will first illustrate the general form of mixed effect models, and generate data based on a given set of design matrices and parameters (\\(X,\\beta, Z, b\\)). In the end, I will set a Bayesian model to estimate the parameters on the simulated data via stan."
  },
  {
    "objectID": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#form",
    "href": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#form",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.1 Form",
    "text": "3.1 Form\n\\[\n\\begin{split}\ny &\\sim \\mathcal{N}(\\mu_n, \\sigma^2) \\\\\n\\mu_n &= \\underbrace{\\alpha_0 + \\beta_0 x_n}_{fixed~~effects} +\\underbrace{\\alpha_{j[n]} + \\beta_{k[n]}x_n}_{random~~effects} \\\\\n\\left(\\begin{array}{c}\\alpha_{j}\\\\ \\beta_{k}\\end{array}\\right) &\\sim  \\mathcal{N}\\left(\\begin{array}{c}\\left(\\begin{array}{c}0\\\\ 0\\end{array}\\right)\\end{array},\\Sigma\\right) \\\\\n\\Sigma &= \\left(\\begin{array}{c}\\sigma_{\\alpha}^2 ~~~ \\rho\\sigma_\\alpha\\sigma_{\\beta}\\\\ \\rho\\sigma_\\alpha\\sigma_{\\beta} ~~~ \\sigma_\\beta^2\\end{array}\\right) \\\\\n&= \\left(\\begin{array}{c}\\sigma_\\alpha ~~ 0\\\\ 0 ~~ \\sigma_\\beta\\end{array}\\right) \\underbrace{\\left(\\begin{array}{c}1 ~~~ \\rho \\\\ \\rho ~~~ 1 \\end{array}\\right)}_{R}  \\left(\\begin{array}{c}\\sigma_\\alpha ~~ 0\\\\ 0 ~~ \\sigma_\\beta\n\\end{array}\\right)\\\\\nR &\\sim \\mathcal{LKJcorr}(2.0) \\\\\n\\alpha_0 & \\sim \\mathcal{N}(0, 10) \\\\\n\\beta_0 & \\sim \\mathcal{N}(0, 10) \\\\\n\\alpha_j & \\sim \\mathcal{N}(0, 10) \\\\\n\\beta_k & \\sim \\mathcal{N}(0, 10) \\\\\n\\sigma & \\sim \\mathcal{N}(0, 5) \\\\\n\\sigma_\\alpha & \\sim \\mathcal{N}(0, 5) \\\\\n\\sigma_\\beta & \\sim \\mathcal{N}(0, 5) \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#model",
    "href": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#model",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.2 Model",
    "text": "3.2 Model"
  },
  {
    "objectID": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#run-the-analysis",
    "href": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#run-the-analysis",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.3 Run the analysis",
    "text": "3.3 Run the analysis\n\nls_final = list(N = nrow(df_final), \n                x = as.vector(df_final$x), \n                y = as.vector(df_final$y),\n                N_group = length(unique(df_final$group_id)), \n                group_id = df_final$group_id)\nlmer_md = stan_model(file = \"./stan_model/lmer_prior_R.stan\")\nfit_lmer_stan = sampling(lmer_md, data = ls_final,\n                         chains = 2, iter = 3000, cores = 2)"
  },
  {
    "objectID": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#traceplot",
    "href": "programming/simulation-based-lmer-stan/simulation-based-lmer-stan.html#traceplot",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.4 Traceplot",
    "text": "3.4 Traceplot\n\nrstan::traceplot(fit_lmer_stan, pars = c(\"alpha_0\", \"beta_0\", \"random_group\"))\n\n\n\n\ntraceplot of posterior parameter estimates"
  },
  {
    "objectID": "programming/coverpage.html#r-programming",
    "href": "programming/coverpage.html#r-programming",
    "title": "Data Science Toolkit",
    "section": "R programming",
    "text": "R programming\n\nColumn-wise and Row-wise Operations in dplyr\n\n\nA Gentle Introduction to Purrr\n\n\nNested vs. Splitted Structures in Data Simulation and Inference"
  },
  {
    "objectID": "statistical_modeling/modeling_statistical_distributions/model_statistical_distributions.html",
    "href": "statistical_modeling/modeling_statistical_distributions/model_statistical_distributions.html",
    "title": "Simulating and Modeling Statistical Distributions via bayes.js",
    "section": "",
    "text": "I have been thinking about building a web app for simulating data with given parameters and recovering the parameters with Bayesian MCMC samplers in JavaScript. This web app can not only make the procedures more transparent, but also help us understand the magic of the Bayesian MCMC approach. More importantly, I have benefited from this simulation-based way of thinking, so I would like to promote it in my blog.\nI looked for some off-the-shelf software online and found a JS library bayes.js developed by Rasmus Bååth. He also wrote a blog to introduce the library. I strongly recommend you first to read his blog to get some key ideas behind it. The library includes an adaptive MCMC sampler (AmwgSampler) in mcmc.js and some common probability distributions in distributions.js. There are also some examples, e.g., you can use bayes.js to fit a Normal distribution and plot the posterior distributions of parameters via plotly.js.\nThis blog heavily relies on Bååth’s library and his implementations. I appreciate his efforts to build a web app for Bayesian data analysis in JavaScript. Here I made some slight adjustments.\n\nInclude the data generating process by using d3.js; You can type console.log(data) or data in your console to check the simulated data.\nSimultaneously update the posterior distributions with the true parameters indicated by red vertical lines;\nEnable and disable buttons and textContent to make it more user-friendly;\nAdd buttons to display messages and warnings.\n\nThese changes are purely based on my personal tastes since I want to simulate data with valid parameters from a probabilistic distribution and check the performance of the posterior estimates against the original parameter values. With some help from ChatGPT, I implemented the web app and deployed it inside my personal webpage powered by Jekyll. Here is the web app for Bayesian MCMC for Normal distributions. If you prefer a standalone version, pls check this page.\n\nSee the Pen  bayes.js normal medium by Yingqi Jing (@jakejing) on CodePen.\n\n\nUseful links:\n\nSimulating and modeling Normal distributions via bayes.js (standalone page)\nHMC algorithm demo\nbayes.js blog"
  },
  {
    "objectID": "statistical_modeling/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html",
    "href": "statistical_modeling/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "Probability masses of normal and lognormal distributions within the corresponding intervals\n\n\n\n\nThe Jacobian adjustment is a key concept in statistical modeling that arises when transforming probability distributions from one space to another. The intuition is that when you change a random variable, you’re not just changing the values - you’re also changing how “stretched” or “compressed” the probability density becomes at different points. To account for the distortion caused by the transform, the density must be multiplied by a Jacobian adjustment. This ensures that probability masses of corresponding intervals stay unchanged before and after the transform. This is illustrated by the figure above, which compares the probability density functions of a standard normal distribution and its transformed lognormal distribution. Although the shapes of the two distributions differ, the transformation must preserve the probability mass over corresponding intervals.\nIn Bayesian inference, Jacobian adjustments are especially important when transforming parameters from a constrained space (e.g., the positive real line) to an unconstrained space (e.g., the entire real line), which is commonly done to improve sampling efficiency and numerical stability. In Stan, such transformations are typically handled automatically. When you declare a constrained parameter (e.g., &lt;lower=0&gt;), Stan internally transforms it to an unconstrained space and applies the appropriate Jacobian adjustment to maintain the correct posterior density.\nHowever, if you manually transform variables inside the transformed parameters block and assign priors to the transformed variables, you need to explicitly include the Jacobian adjustment to preserve the correct log posterior density (lp__). Failing to do so can lead to biased inference. To illustrate how this works, I’ll use a simple example of normal distribution, focusing on how Stan handles transformations of the standard deviation parameter \\(\\sigma\\) (i.e., &lt;lower=0&gt;) and how we can include a manual Jacobian adjustment.\n\n\nI will first simulate some data from a normal distribution with mean 0 and standard deviation 20. A large standard deviation is chosen to make the effect of Jacobian adjustment more pronounced.\n\ndata_norm &lt;- list(N = 100, y = rnorm(100, 0, 20))\n\n\n\n\nI will include four models to compare posterior parameter estimates and the log posterior density (lp__). The models are:\nModel 1: A normal distribution with a proper constraint on \\(\\sigma\\) Model 2: A normal distribution without a constraint on \\(\\sigma\\) Model 3: A normal distribution with an exponential transformation of unconstrained \\(\\sigma_z\\) and a Jacobian adjustment Model 4: A normal distribution with transformation of \\(\\sigma\\) in the transformed parameters block (No Jacobian needed!)\n\n\nThe first model is a simple normal distribution with a proper constraint on \\(\\sigma\\) (&lt;lower=0&gt;). According to Stan reference manual, to avoid having to deal with constraints while simulating the Hamiltonian dynamics during sampling, every (multivariate) parameter in a Stan model is transformed to an unconstrained variable behind the scenes by the model compiler, and Stan handles the Jacobian adjustment automatically.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real&lt;lower=0&gt; sigma; // standard deviation parameter\n}\n\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm &lt;- stan_model(file = \"./Models/normal.stan\")\nfit_norm &lt;- sampling(md_norm, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nNow we turn to another model by removing the constraint on \\(\\sigma\\). In this case, the parameter \\(\\sigma\\) is not a constrained variable, and there is no Jacobian adjustment handled by Stan. This means that the log posterior density (lp__) is biased.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real sigma; // standard deviation parameter\n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm_no_constraint &lt;- stan_model(file = \"./Models/normal_no_constraint_sigma.stan\")\nfit_norm_no_constraint &lt;- sampling(md_norm_no_constraint, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_no_constraint, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs a comparison, we can also reformulate the model by defining the parameter \\(\\sigma_z\\) as an unconstrained variable, and we then transform it via an exponential function (positive real line). The transformed variable \\(\\sigma\\) will be assigned with a prior and used in the model. This is exactly what has happened internally by Stan when you define a parameter with a proper constraint (e.g., &lt;lower=0&gt; for \\(\\sigma\\)). Stan handles the transformation from an unconstrained internal representation to this constrained user-facing value. Since \\(\\sigma\\) is transformed from \\(\\sigma_z\\), we need to include a Jacobian adjustment to preserve the correct log posterior density (lp__).\n\nLet me explain how the Jacobian adjustment works step by step.\nLet: \\[y = \\sigma_e, \\quad x = \\sigma, \\quad y = \\exp(x)\\]\nWe are transforming from an unconstrained variable \\(x \\in \\mathbb{R}\\) to a positive variable \\(y \\in (0, \\infty)\\).\nNext, we can compute the derivative: \\[\\frac{dy}{dx} = \\frac{d}{dx} \\exp(x) = \\exp(x) = y\\]\nWe apply the change-of-variables formula for densities: \\[\\left|f_Y(y) \\cdot dy\\right| = \\left|f_X(x) \\cdot dx\\right|\n\\quad \\Rightarrow \\quad\nf_Y(y) \\cdot \\left| \\frac{dy}{dx} \\right| = f_X(x)\\]\nSubstituting \\(\\frac{dy}{dx} = y\\), we get: \\[f_Y(y) \\cdot y = f_X(x)\\]\nTaking logs to get log-densities: \\[\\log f_X(x) = \\log f_Y(y) + \\log y\\]\nThis extra term \\(\\log y\\) is the Jacobian adjustment.\nIn Stan notation, we get:\n\\[\\text{target} ~ \\text{+=} ~ \\text{normal\\_lpdf}(\\mu, \\exp(\\sigma_e)) + \\log(\\sigma_e)\\]\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\nparameters {\n  real mu; // mean parameter\n  real sigma_z; // unconstrained standard deviation parameter\n}\ntransformed parameters {\n  real sigma = exp(sigma_z);\n}\nmodel {\n  // Method 1: prior on sigma, with transformed block and Jacobian adjustment\n  target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  // Method 2: local variable sigma, no transformed block, but with Jacobian adjustment\n  // real sigma = exp(sigma_z);\n  // target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma) + log(sigma); // add Jacobian adjustment\n  // target += normal_lpdf(y | mu, sigma) + sigma_z; // alternatively\n}\n\n\nmd_norm_exp_jacobian &lt;- stan_model(file = \"./Models/normal_exp_sigma_jacobian.stan\")\nfit_norm_exp_jacobian &lt;- sampling(md_norm_exp_jacobian, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"lp__\"))\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nIt is also worth mentioning that if you transform the parameter \\(\\sigma\\) in the transformed parameters block without assigning a prior to the transformed parameter, you do not need to include a Jacobian adjustment. This is because the transformation is applied to the parameter after sampling. This is conceptually similar to generating quantities from posterior draws.\nAs a general rule, if you place priors on the declared parameters or directly use the parameters inside parameters block (in most cases), rather than on transformed parameters, no Jacobian adjustment is needed—this is a simple variable transformation. By contrast, if you transform a parameter and place a prior on the transformed variable, you need to include a Jacobian adjustment.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n} \ntransformed parameters {\n  // Method 1: simple transformation without a prior for the transformed parameter\n  real log_sigma = log(sigma); \n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(sigma | 0, 5); // prior on sigma\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_parameters &lt;- stan_model(file = \"./Models/normal_transform_parameters.stan\")\nfit_norm_transform_parameters &lt;- sampling(md_norm_transform_parameters, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_parameters, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nYou may think of it in a different way by transforming the parameter \\(\\sigma\\) via logrithm transformation. This is not what happened under the hood in stan, since Jacobian adjustment is performed on the absolute derivative of the inverse transform. See the stan reference manual for more details.\n\nUnivariate changes of variables Suppose \\(X\\) is one dimensional and \\(f : \\mathrm{supp}(X) \\to \\mathbb{R}\\) is a one-to-one, monotonic function with a differentiable inverse \\(f^{-1}\\). Then the density of \\(Y\\) is given by\n\\[p_Y(y) = p_X(f^{-1}(y)) \\left| \\frac{d}{dy} f^{-1}(y) \\right|\\]\nThe absolute derivative of the inverse transform measures how the scale of the transformed variable changes with respect to the underlying variable.\n\nIf you change in this way, you will change the prior on \\(\\sigma\\). You will not get the same log posterior density (lp__) as Model 1, since the prior on \\(\\sigma\\) is different.\nIn model 1: \\(\\sigma \\sim \\mathcal{N}(0, 5)\\)\nIn model 5: \\(\\log(\\sigma) \\sim \\mathcal{N}(0, 5)\\) or \\(\\sigma \\sim \\mathcal{LogN}(0, 5)\\)\nMy own opinion is that it is not recommended to transform the parameter locally inside the model block, since (1) it is not that transparent unless you really know what you are doing and (2) it will not be saved in the output.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Method 1: prior on log(sigma) --&gt; lead to a different prior on sigma\n  target += normal_lpdf(log(sigma) | 0, 5);\n  target += lognormal_lpdf(sigma | 0, 5); // equivalently\n\n  // Method 2: prior on local variable sigma_log with Jacobian adjustment\n  // real sigma_log = log(sigma);\n  // target += normal_lpdf(sigma_log | 0, 5);\n\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_local &lt;- stan_model(file = \"./Models/normal_transform_local.stan\")\nfit_norm_transform_local &lt;- sampling(md_norm_transform_local, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_local, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs we can see, the posterior parameter estimates for \\(\\mu\\) and \\(\\sigma\\) are similar across all three models. However, the log posterior density (lp__) differs between Model 1 and Model 2. This is because Model 1 includes the proper constraint on \\(\\sigma\\), while Model 2 does not. The log posterior density in Model 2 is biased due to the missing Jacobian adjustment. Model 3 addresses this issue by including a Jacobian adjustment. In general, if you are interested in parameter inference, it may be not a major concern in this case, but missing Jacobian adjustments can cause serious problems for model comparison (e.g., WAIC, LOO, and Bayes factors).\nNote that this example is only for illustration and help you understand the concept of Jacobian adjustment and how Stan handles changes of variables. In practice, you should always use the proper constraint on the parameter and let Stan handle the Jacobian adjustment automatically, which is both more efficient and more reliable.\nRelated links\n\n(Best) A coin toss example with Jacobian transformation\nThe Jacobian transformation\nChanges of variables\nTransforms\nLaplace method and Jacobian of parameter transformation"
  },
  {
    "objectID": "statistical_modeling/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#simulate-data",
    "href": "statistical_modeling/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#simulate-data",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "I will first simulate some data from a normal distribution with mean 0 and standard deviation 20. A large standard deviation is chosen to make the effect of Jacobian adjustment more pronounced.\n\ndata_norm &lt;- list(N = 100, y = rnorm(100, 0, 20))"
  },
  {
    "objectID": "statistical_modeling/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#posterior-parameter-estimates",
    "href": "statistical_modeling/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#posterior-parameter-estimates",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "I will include four models to compare posterior parameter estimates and the log posterior density (lp__). The models are:\nModel 1: A normal distribution with a proper constraint on \\(\\sigma\\) Model 2: A normal distribution without a constraint on \\(\\sigma\\) Model 3: A normal distribution with an exponential transformation of unconstrained \\(\\sigma_z\\) and a Jacobian adjustment Model 4: A normal distribution with transformation of \\(\\sigma\\) in the transformed parameters block (No Jacobian needed!)\n\n\nThe first model is a simple normal distribution with a proper constraint on \\(\\sigma\\) (&lt;lower=0&gt;). According to Stan reference manual, to avoid having to deal with constraints while simulating the Hamiltonian dynamics during sampling, every (multivariate) parameter in a Stan model is transformed to an unconstrained variable behind the scenes by the model compiler, and Stan handles the Jacobian adjustment automatically.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real&lt;lower=0&gt; sigma; // standard deviation parameter\n}\n\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm &lt;- stan_model(file = \"./Models/normal.stan\")\nfit_norm &lt;- sampling(md_norm, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nNow we turn to another model by removing the constraint on \\(\\sigma\\). In this case, the parameter \\(\\sigma\\) is not a constrained variable, and there is no Jacobian adjustment handled by Stan. This means that the log posterior density (lp__) is biased.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real sigma; // standard deviation parameter\n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm_no_constraint &lt;- stan_model(file = \"./Models/normal_no_constraint_sigma.stan\")\nfit_norm_no_constraint &lt;- sampling(md_norm_no_constraint, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_no_constraint, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs a comparison, we can also reformulate the model by defining the parameter \\(\\sigma_z\\) as an unconstrained variable, and we then transform it via an exponential function (positive real line). The transformed variable \\(\\sigma\\) will be assigned with a prior and used in the model. This is exactly what has happened internally by Stan when you define a parameter with a proper constraint (e.g., &lt;lower=0&gt; for \\(\\sigma\\)). Stan handles the transformation from an unconstrained internal representation to this constrained user-facing value. Since \\(\\sigma\\) is transformed from \\(\\sigma_z\\), we need to include a Jacobian adjustment to preserve the correct log posterior density (lp__).\n\nLet me explain how the Jacobian adjustment works step by step.\nLet: \\[y = \\sigma_e, \\quad x = \\sigma, \\quad y = \\exp(x)\\]\nWe are transforming from an unconstrained variable \\(x \\in \\mathbb{R}\\) to a positive variable \\(y \\in (0, \\infty)\\).\nNext, we can compute the derivative: \\[\\frac{dy}{dx} = \\frac{d}{dx} \\exp(x) = \\exp(x) = y\\]\nWe apply the change-of-variables formula for densities: \\[\\left|f_Y(y) \\cdot dy\\right| = \\left|f_X(x) \\cdot dx\\right|\n\\quad \\Rightarrow \\quad\nf_Y(y) \\cdot \\left| \\frac{dy}{dx} \\right| = f_X(x)\\]\nSubstituting \\(\\frac{dy}{dx} = y\\), we get: \\[f_Y(y) \\cdot y = f_X(x)\\]\nTaking logs to get log-densities: \\[\\log f_X(x) = \\log f_Y(y) + \\log y\\]\nThis extra term \\(\\log y\\) is the Jacobian adjustment.\nIn Stan notation, we get:\n\\[\\text{target} ~ \\text{+=} ~ \\text{normal\\_lpdf}(\\mu, \\exp(\\sigma_e)) + \\log(\\sigma_e)\\]\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\nparameters {\n  real mu; // mean parameter\n  real sigma_z; // unconstrained standard deviation parameter\n}\ntransformed parameters {\n  real sigma = exp(sigma_z);\n}\nmodel {\n  // Method 1: prior on sigma, with transformed block and Jacobian adjustment\n  target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  // Method 2: local variable sigma, no transformed block, but with Jacobian adjustment\n  // real sigma = exp(sigma_z);\n  // target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma) + log(sigma); // add Jacobian adjustment\n  // target += normal_lpdf(y | mu, sigma) + sigma_z; // alternatively\n}\n\n\nmd_norm_exp_jacobian &lt;- stan_model(file = \"./Models/normal_exp_sigma_jacobian.stan\")\nfit_norm_exp_jacobian &lt;- sampling(md_norm_exp_jacobian, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"lp__\"))\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nIt is also worth mentioning that if you transform the parameter \\(\\sigma\\) in the transformed parameters block without assigning a prior to the transformed parameter, you do not need to include a Jacobian adjustment. This is because the transformation is applied to the parameter after sampling. This is conceptually similar to generating quantities from posterior draws.\nAs a general rule, if you place priors on the declared parameters or directly use the parameters inside parameters block (in most cases), rather than on transformed parameters, no Jacobian adjustment is needed—this is a simple variable transformation. By contrast, if you transform a parameter and place a prior on the transformed variable, you need to include a Jacobian adjustment.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n} \ntransformed parameters {\n  // Method 1: simple transformation without a prior for the transformed parameter\n  real log_sigma = log(sigma); \n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(sigma | 0, 5); // prior on sigma\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_parameters &lt;- stan_model(file = \"./Models/normal_transform_parameters.stan\")\nfit_norm_transform_parameters &lt;- sampling(md_norm_transform_parameters, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_parameters, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nYou may think of it in a different way by transforming the parameter \\(\\sigma\\) via logrithm transformation. This is not what happened under the hood in stan, since Jacobian adjustment is performed on the absolute derivative of the inverse transform. See the stan reference manual for more details.\n\nUnivariate changes of variables Suppose \\(X\\) is one dimensional and \\(f : \\mathrm{supp}(X) \\to \\mathbb{R}\\) is a one-to-one, monotonic function with a differentiable inverse \\(f^{-1}\\). Then the density of \\(Y\\) is given by\n\\[p_Y(y) = p_X(f^{-1}(y)) \\left| \\frac{d}{dy} f^{-1}(y) \\right|\\]\nThe absolute derivative of the inverse transform measures how the scale of the transformed variable changes with respect to the underlying variable.\n\nIf you change in this way, you will change the prior on \\(\\sigma\\). You will not get the same log posterior density (lp__) as Model 1, since the prior on \\(\\sigma\\) is different.\nIn model 1: \\(\\sigma \\sim \\mathcal{N}(0, 5)\\)\nIn model 5: \\(\\log(\\sigma) \\sim \\mathcal{N}(0, 5)\\) or \\(\\sigma \\sim \\mathcal{LogN}(0, 5)\\)\nMy own opinion is that it is not recommended to transform the parameter locally inside the model block, since (1) it is not that transparent unless you really know what you are doing and (2) it will not be saved in the output.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Method 1: prior on log(sigma) --&gt; lead to a different prior on sigma\n  target += normal_lpdf(log(sigma) | 0, 5);\n  target += lognormal_lpdf(sigma | 0, 5); // equivalently\n\n  // Method 2: prior on local variable sigma_log with Jacobian adjustment\n  // real sigma_log = log(sigma);\n  // target += normal_lpdf(sigma_log | 0, 5);\n\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_local &lt;- stan_model(file = \"./Models/normal_transform_local.stan\")\nfit_norm_transform_local &lt;- sampling(md_norm_transform_local, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_local, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs we can see, the posterior parameter estimates for \\(\\mu\\) and \\(\\sigma\\) are similar across all three models. However, the log posterior density (lp__) differs between Model 1 and Model 2. This is because Model 1 includes the proper constraint on \\(\\sigma\\), while Model 2 does not. The log posterior density in Model 2 is biased due to the missing Jacobian adjustment. Model 3 addresses this issue by including a Jacobian adjustment. In general, if you are interested in parameter inference, it may be not a major concern in this case, but missing Jacobian adjustments can cause serious problems for model comparison (e.g., WAIC, LOO, and Bayes factors).\nNote that this example is only for illustration and help you understand the concept of Jacobian adjustment and how Stan handles changes of variables. In practice, you should always use the proper constraint on the parameter and let Stan handle the Jacobian adjustment automatically, which is both more efficient and more reliable.\nRelated links\n\n(Best) A coin toss example with Jacobian transformation\nThe Jacobian transformation\nChanges of variables\nTransforms\nLaplace method and Jacobian of parameter transformation"
  },
  {
    "objectID": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html",
    "href": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "",
    "text": "In this blog, I will provide a step-by-step instruction of how we can generate data from a mixed effect model and recover the parameters of the model with the simulated dataset. This simulation-based experiment can help us better understand the structure and generative process of the multilevel model with correlated random intercepts and slopes. To proceed, I will first illustrate the general form of mixed effect models, and generate data based on a given set of design matrices and parameters (\\(X,\\beta, Z, b\\)). In the end, I will set a Bayesian model to estimate the parameters on the simulated data via stan."
  },
  {
    "objectID": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#form",
    "href": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#form",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.1 Form",
    "text": "3.1 Form\n\\[\n\\begin{split}\ny &\\sim \\mathcal{N}(\\mu_n, \\sigma^2) \\\\\n\\mu_n &= \\underbrace{\\alpha_0 + \\beta_0 x_n}_{fixed~~effects} +\\underbrace{\\alpha_{j[n]} + \\beta_{k[n]}x_n}_{random~~effects} \\\\\n\\left(\\begin{array}{c}\\alpha_{j}\\\\ \\beta_{k}\\end{array}\\right) &\\sim  \\mathcal{N}\\left(\\begin{array}{c}\\left(\\begin{array}{c}0\\\\ 0\\end{array}\\right)\\end{array},\\Sigma\\right) \\\\\n\\Sigma &= \\left(\\begin{array}{c}\\sigma_{\\alpha}^2 ~~~ \\rho\\sigma_\\alpha\\sigma_{\\beta}\\\\ \\rho\\sigma_\\alpha\\sigma_{\\beta} ~~~ \\sigma_\\beta^2\\end{array}\\right) \\\\\n&= \\left(\\begin{array}{c}\\sigma_\\alpha ~~ 0\\\\ 0 ~~ \\sigma_\\beta\\end{array}\\right) \\underbrace{\\left(\\begin{array}{c}1 ~~~ \\rho \\\\ \\rho ~~~ 1 \\end{array}\\right)}_{R}  \\left(\\begin{array}{c}\\sigma_\\alpha ~~ 0\\\\ 0 ~~ \\sigma_\\beta\n\\end{array}\\right)\\\\\nR &\\sim \\mathcal{LKJcorr}(2.0) \\\\\n\\alpha_0 & \\sim \\mathcal{N}(0, 10) \\\\\n\\beta_0 & \\sim \\mathcal{N}(0, 10) \\\\\n\\alpha_j & \\sim \\mathcal{N}(0, 10) \\\\\n\\beta_k & \\sim \\mathcal{N}(0, 10) \\\\\n\\sigma & \\sim \\mathcal{N}(0, 5) \\\\\n\\sigma_\\alpha & \\sim \\mathcal{N}(0, 5) \\\\\n\\sigma_\\beta & \\sim \\mathcal{N}(0, 5) \\\\\n\\end{split}\n\\]"
  },
  {
    "objectID": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#model",
    "href": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#model",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.2 Model",
    "text": "3.2 Model"
  },
  {
    "objectID": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#run-the-analysis",
    "href": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#run-the-analysis",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.3 Run the analysis",
    "text": "3.3 Run the analysis\n\nls_final = list(N = nrow(df_final), \n                x = as.vector(df_final$x), \n                y = as.vector(df_final$y),\n                N_group = length(unique(df_final$group_id)), \n                group_id = df_final$group_id)\nlmer_md = stan_model(file = \"./stan_model/lmer_prior_R.stan\")\nfit_lmer_stan = sampling(lmer_md, data = ls_final,\n                         chains = 2, iter = 3000, cores = 2)"
  },
  {
    "objectID": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#traceplot",
    "href": "statistical_modeling/simulation-based-lmer-stan/simulation-based-lmer-stan.html#traceplot",
    "title": "Simulation-based Linear Mixed-effects Models with Stan",
    "section": "3.4 Traceplot",
    "text": "3.4 Traceplot\n\nrstan::traceplot(fit_lmer_stan, pars = c(\"alpha_0\", \"beta_0\", \"random_group\"))\n\n\n\n\ntraceplot of posterior parameter estimates"
  },
  {
    "objectID": "statistical_modeling/coverpage.html",
    "href": "statistical_modeling/coverpage.html",
    "title": "Data Science Toolkit",
    "section": "",
    "text": "Understanding Jacobian Adjustments for Constrained Parameters in Stan\n\n\nSimulation-based Linear Mixed-effects Models with Stan\n\n\nSimulating and Modeling Statistical Distributions via bayes.js"
  }
]