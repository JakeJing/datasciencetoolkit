[
  {
    "objectID": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html",
    "href": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "Probability masses of normal and lognormal distributions within the corresponding intervals\n\n\n\n\nThe Jacobian adjustment is a key concept in statistical modeling that arises when transforming probability distributions from one space to another. The intuition is that when you change a random variable, you’re not just changing the values - you’re also changing how “stretched” or “compressed” the probability density becomes at different points. To account for the distortion caused by the transform, the density must be multiplied by a Jacobian adjustment. This ensures that probability masses of corresponding intervals stay unchanged before and after the transform. This is illustrated by the figure above, which compares the probability density functions of a standard normal distribution and its transformed lognormal distribution. Although the shapes of the two distributions differ, the transformation must preserve the probability mass over corresponding intervals.\nIn Bayesian inference, Jacobian adjustments are especially important when transforming parameters from a constrained space (e.g., the positive real line) to an unconstrained space (e.g., the entire real line), which is commonly done to improve sampling efficiency and numerical stability. In Stan, such transformations are typically handled automatically. When you declare a constrained parameter (e.g., &lt;lower=0&gt;), Stan internally transforms it to an unconstrained space and applies the appropriate Jacobian adjustment to maintain the correct posterior density.\nHowever, if you manually transform variables inside the transformed parameters block and assign priors to the transformed variables, you need to explicitly include the Jacobian adjustment to preserve the correct log posterior density (lp__). Failing to do so can lead to biased inference. To illustrate how this works, I’ll use a simple example of normal distribution, focusing on how Stan handles transformations of the standard deviation parameter \\(\\sigma\\) (i.e., &lt;lower=0&gt;) and how we can include a manual Jacobian adjustment.\n\n\nI will first simulate some data from a normal distribution with mean 0 and standard deviation 20. A large standard deviation is chosen to make the effect of Jacobian adjustment more pronounced.\n\ndata_norm &lt;- list(N = 100, y = rnorm(100, 0, 20))\n\n\n\n\nI will include four models to compare posterior parameter estimates and the log posterior density (lp__). The models are:\nModel 1: A normal distribution with a proper constraint on \\(\\sigma\\) Model 2: A normal distribution without a constraint on \\(\\sigma\\) Model 3: A normal distribution with an exponential transformation of unconstrained \\(\\sigma_z\\) and a Jacobian adjustment Model 4: A normal distribution with transformation of \\(\\sigma\\) in the transformed parameters block (No Jacobian needed!)\n\n\nThe first model is a simple normal distribution with a proper constraint on \\(\\sigma\\) (&lt;lower=0&gt;). According to Stan reference manual, to avoid having to deal with constraints while simulating the Hamiltonian dynamics during sampling, every (multivariate) parameter in a Stan model is transformed to an unconstrained variable behind the scenes by the model compiler, and Stan handles the Jacobian adjustment automatically.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real&lt;lower=0&gt; sigma; // standard deviation parameter\n}\n\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm &lt;- stan_model(file = \"./Models/normal.stan\")\nfit_norm &lt;- sampling(md_norm, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nNow we turn to another model by removing the constraint on \\(\\sigma\\). In this case, the parameter \\(\\sigma\\) is not a constrained variable, and there is no Jacobian adjustment handled by Stan. This means that the log posterior density (lp__) is biased.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real sigma; // standard deviation parameter\n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm_no_constraint &lt;- stan_model(file = \"./Models/normal_no_constraint_sigma.stan\")\nfit_norm_no_constraint &lt;- sampling(md_norm_no_constraint, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_no_constraint, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs a comparison, we can also reformulate the model by defining the parameter \\(\\sigma_z\\) as an unconstrained variable, and we then transform it via an exponential function (positive real line). The transformed variable \\(\\sigma\\) will be assigned with a prior and used in the model. This is exactly what has happened internally by Stan when you define a parameter with a proper constraint (e.g., &lt;lower=0&gt; for \\(\\sigma\\)). Stan handles the transformation from an unconstrained internal representation to this constrained user-facing value. Since \\(\\sigma\\) is transformed from \\(\\sigma_z\\), we need to include a Jacobian adjustment to preserve the correct log posterior density (lp__).\n\nLet me explain how the Jacobian adjustment works step by step.\nLet: \\[y = \\sigma_e, \\quad x = \\sigma, \\quad y = \\exp(x)\\]\nWe are transforming from an unconstrained variable \\(x \\in \\mathbb{R}\\) to a positive variable \\(y \\in (0, \\infty)\\).\nNext, we can compute the derivative: \\[\\frac{dy}{dx} = \\frac{d}{dx} \\exp(x) = \\exp(x) = y\\]\nWe apply the change-of-variables formula for densities: \\[\\left|f_Y(y) \\cdot dy\\right| = \\left|f_X(x) \\cdot dx\\right|\n\\quad \\Rightarrow \\quad\nf_Y(y) \\cdot \\left| \\frac{dy}{dx} \\right| = f_X(x)\\]\nSubstituting \\(\\frac{dy}{dx} = y\\), we get: \\[f_Y(y) \\cdot y = f_X(x)\\]\nTaking logs to get log-densities: \\[\\log f_X(x) = \\log f_Y(y) + \\log y\\]\nThis extra term \\(\\log y\\) is the Jacobian adjustment.\nIn Stan notation, we get:\n\\[\\text{target} ~ \\text{+=} ~ \\text{normal\\_lpdf}(\\mu, \\exp(\\sigma_e)) + \\log(\\sigma_e)\\]\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\nparameters {\n  real mu; // mean parameter\n  real sigma_z; // unconstrained standard deviation parameter\n}\ntransformed parameters {\n  real sigma = exp(sigma_z);\n}\nmodel {\n  // Method 1: prior on sigma, with transformed block and Jacobian adjustment\n  target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  // Method 2: local variable sigma, no transformed block, but with Jacobian adjustment\n  // real sigma = exp(sigma_z);\n  // target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma) + log(sigma); // add Jacobian adjustment\n  // target += normal_lpdf(y | mu, sigma) + sigma_z; // alternatively\n}\n\n\nmd_norm_exp_jacobian &lt;- stan_model(file = \"./Models/normal_exp_sigma_jacobian.stan\")\nfit_norm_exp_jacobian &lt;- sampling(md_norm_exp_jacobian, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"lp__\"))\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nIt is also worth mentioning that if you transform the parameter \\(\\sigma\\) in the transformed parameters block without assigning a prior to the transformed parameter, you do not need to include a Jacobian adjustment. This is because the transformation is applied to the parameter after sampling. This is conceptually similar to generating quantities from posterior draws.\nAs a general rule, if you place priors on the declared parameters or directly use the parameters inside parameters block (in most cases), rather than on transformed parameters, no Jacobian adjustment is needed—this is a simple variable transformation. By contrast, if you transform a parameter and place a prior on the transformed variable, you need to include a Jacobian adjustment.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n} \ntransformed parameters {\n  // Method 1: simple transformation without a prior for the transformed parameter\n  real log_sigma = log(sigma); \n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(sigma | 0, 5); // prior on sigma\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_parameters &lt;- stan_model(file = \"./Models/normal_transform_parameters.stan\")\nfit_norm_transform_parameters &lt;- sampling(md_norm_transform_parameters, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_parameters, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nYou may think of it in a different way by transforming the parameter \\(\\sigma\\) via logrithm transformation. This is not what happened under the hood in stan, since Jacobian adjustment is performed on the absolute derivative of the inverse transform. See the stan reference manual for more details.\n\nUnivariate changes of variables Suppose \\(X\\) is one dimensional and \\(f : \\mathrm{supp}(X) \\to \\mathbb{R}\\) is a one-to-one, monotonic function with a differentiable inverse \\(f^{-1}\\). Then the density of \\(Y\\) is given by\n\\[p_Y(y) = p_X(f^{-1}(y)) \\left| \\frac{d}{dy} f^{-1}(y) \\right|\\]\nThe absolute derivative of the inverse transform measures how the scale of the transformed variable changes with respect to the underlying variable.\n\nIf you change in this way, you will change the prior on \\(\\sigma\\). You will not get the same log posterior density (lp__) as Model 1, since the prior on \\(\\sigma\\) is different.\nIn model 1: \\(\\sigma \\sim \\mathcal{N}(0, 5)\\)\nIn model 5: \\(\\log(\\sigma) \\sim \\mathcal{N}(0, 5)\\) or \\(\\sigma \\sim \\mathcal{LogN}(0, 5)\\)\nMy own opinion is that it is not recommended to transform the parameter locally inside the model block, since (1) it is not that transparent unless you really know what you are doing and (2) it will not be saved in the output.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Method 1: prior on log(sigma) --&gt; lead to a different prior on sigma\n  target += normal_lpdf(log(sigma) | 0, 5);\n  target += lognormal_lpdf(sigma | 0, 5); // equivalently\n\n  // Method 2: prior on local variable sigma_log with Jacobian adjustment\n  // real sigma_log = log(sigma);\n  // target += normal_lpdf(sigma_log | 0, 5);\n\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_local &lt;- stan_model(file = \"./Models/normal_transform_local.stan\")\nfit_norm_transform_local &lt;- sampling(md_norm_transform_local, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_local, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs we can see, the posterior parameter estimates for \\(\\mu\\) and \\(\\sigma\\) are similar across all three models. However, the log posterior density (lp__) differs between Model 1 and Model 2. This is because Model 1 includes the proper constraint on \\(\\sigma\\), while Model 2 does not. The log posterior density in Model 2 is biased due to the missing Jacobian adjustment. Model 3 addresses this issue by including a Jacobian adjustment. In general, if you are interested in parameter inference, it may be not a major concern in this case, but missing Jacobian adjustments can cause serious problems for model comparison (e.g., WAIC, LOO, and Bayes factors).\nNote that this example is only for illustration and help you understand the concept of Jacobian adjustment and how Stan handles changes of variables. In practice, you should always use the proper constraint on the parameter and let Stan handle the Jacobian adjustment automatically, which is both more efficient and more reliable.\nRelated links\n\n(Best) A coin toss example with Jacobian transformation\nThe Jacobian transformation\nChanges of variables\nTransforms\nLaplace method and Jacobian of parameter transformation"
  },
  {
    "objectID": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#simulate-data",
    "href": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#simulate-data",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "I will first simulate some data from a normal distribution with mean 0 and standard deviation 20. A large standard deviation is chosen to make the effect of Jacobian adjustment more pronounced.\n\ndata_norm &lt;- list(N = 100, y = rnorm(100, 0, 20))"
  },
  {
    "objectID": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#posterior-parameter-estimates",
    "href": "programming/Jacobian-adjustment-normal/understanding-jacobian-adjustment.html#posterior-parameter-estimates",
    "title": "Understanding Jacobian Adjustments for Constrained Parameters in Stan",
    "section": "",
    "text": "I will include four models to compare posterior parameter estimates and the log posterior density (lp__). The models are:\nModel 1: A normal distribution with a proper constraint on \\(\\sigma\\) Model 2: A normal distribution without a constraint on \\(\\sigma\\) Model 3: A normal distribution with an exponential transformation of unconstrained \\(\\sigma_z\\) and a Jacobian adjustment Model 4: A normal distribution with transformation of \\(\\sigma\\) in the transformed parameters block (No Jacobian needed!)\n\n\nThe first model is a simple normal distribution with a proper constraint on \\(\\sigma\\) (&lt;lower=0&gt;). According to Stan reference manual, to avoid having to deal with constraints while simulating the Hamiltonian dynamics during sampling, every (multivariate) parameter in a Stan model is transformed to an unconstrained variable behind the scenes by the model compiler, and Stan handles the Jacobian adjustment automatically.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real&lt;lower=0&gt; sigma; // standard deviation parameter\n}\n\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm &lt;- stan_model(file = \"./Models/normal.stan\")\nfit_norm &lt;- sampling(md_norm, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nNow we turn to another model by removing the constraint on \\(\\sigma\\). In this case, the parameter \\(\\sigma\\) is not a constrained variable, and there is no Jacobian adjustment handled by Stan. This means that the log posterior density (lp__) is biased.\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\n\nparameters {\n  real mu; // mean parameter\n  real sigma; // standard deviation parameter\n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  target += normal_lpdf(sigma | 0, 5); // prior for standard deviation\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma); // data follows normal distribution\n}\n\nmd_norm_no_constraint &lt;- stan_model(file = \"./Models/normal_no_constraint_sigma.stan\")\nfit_norm_no_constraint &lt;- sampling(md_norm_no_constraint, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_no_constraint, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs a comparison, we can also reformulate the model by defining the parameter \\(\\sigma_z\\) as an unconstrained variable, and we then transform it via an exponential function (positive real line). The transformed variable \\(\\sigma\\) will be assigned with a prior and used in the model. This is exactly what has happened internally by Stan when you define a parameter with a proper constraint (e.g., &lt;lower=0&gt; for \\(\\sigma\\)). Stan handles the transformation from an unconstrained internal representation to this constrained user-facing value. Since \\(\\sigma\\) is transformed from \\(\\sigma_z\\), we need to include a Jacobian adjustment to preserve the correct log posterior density (lp__).\n\nLet me explain how the Jacobian adjustment works step by step.\nLet: \\[y = \\sigma_e, \\quad x = \\sigma, \\quad y = \\exp(x)\\]\nWe are transforming from an unconstrained variable \\(x \\in \\mathbb{R}\\) to a positive variable \\(y \\in (0, \\infty)\\).\nNext, we can compute the derivative: \\[\\frac{dy}{dx} = \\frac{d}{dx} \\exp(x) = \\exp(x) = y\\]\nWe apply the change-of-variables formula for densities: \\[\\left|f_Y(y) \\cdot dy\\right| = \\left|f_X(x) \\cdot dx\\right|\n\\quad \\Rightarrow \\quad\nf_Y(y) \\cdot \\left| \\frac{dy}{dx} \\right| = f_X(x)\\]\nSubstituting \\(\\frac{dy}{dx} = y\\), we get: \\[f_Y(y) \\cdot y = f_X(x)\\]\nTaking logs to get log-densities: \\[\\log f_X(x) = \\log f_Y(y) + \\log y\\]\nThis extra term \\(\\log y\\) is the Jacobian adjustment.\nIn Stan notation, we get:\n\\[\\text{target} ~ \\text{+=} ~ \\text{normal\\_lpdf}(\\mu, \\exp(\\sigma_e)) + \\log(\\sigma_e)\\]\n\ndata {\n  int&lt;lower=0&gt; N; // number of observations\n  vector[N] y; // observed data\n}\nparameters {\n  real mu; // mean parameter\n  real sigma_z; // unconstrained standard deviation parameter\n}\ntransformed parameters {\n  real sigma = exp(sigma_z);\n}\nmodel {\n  // Method 1: prior on sigma, with transformed block and Jacobian adjustment\n  target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  // Method 2: local variable sigma, no transformed block, but with Jacobian adjustment\n  // real sigma = exp(sigma_z);\n  // target += normal_lpdf(sigma | 0, 5); // prior for the transformed standard deviation\n\n  target += normal_lpdf(mu | 0, 10); // prior for mean\n  \n  // Likelihood\n  target += normal_lpdf(y | mu, sigma) + log(sigma); // add Jacobian adjustment\n  // target += normal_lpdf(y | mu, sigma) + sigma_z; // alternatively\n}\n\n\nmd_norm_exp_jacobian &lt;- stan_model(file = \"./Models/normal_exp_sigma_jacobian.stan\")\nfit_norm_exp_jacobian &lt;- sampling(md_norm_exp_jacobian, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"lp__\"))\nprint(fit_norm_exp_jacobian, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nIt is also worth mentioning that if you transform the parameter \\(\\sigma\\) in the transformed parameters block without assigning a prior to the transformed parameter, you do not need to include a Jacobian adjustment. This is because the transformation is applied to the parameter after sampling. This is conceptually similar to generating quantities from posterior draws.\nAs a general rule, if you place priors on the declared parameters or directly use the parameters inside parameters block (in most cases), rather than on transformed parameters, no Jacobian adjustment is needed—this is a simple variable transformation. By contrast, if you transform a parameter and place a prior on the transformed variable, you need to include a Jacobian adjustment.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n} \ntransformed parameters {\n  // Method 1: simple transformation without a prior for the transformed parameter\n  real log_sigma = log(sigma); \n}\nmodel {\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(sigma | 0, 5); // prior on sigma\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_parameters &lt;- stan_model(file = \"./Models/normal_transform_parameters.stan\")\nfit_norm_transform_parameters &lt;- sampling(md_norm_transform_parameters, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_parameters, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nYou may think of it in a different way by transforming the parameter \\(\\sigma\\) via logrithm transformation. This is not what happened under the hood in stan, since Jacobian adjustment is performed on the absolute derivative of the inverse transform. See the stan reference manual for more details.\n\nUnivariate changes of variables Suppose \\(X\\) is one dimensional and \\(f : \\mathrm{supp}(X) \\to \\mathbb{R}\\) is a one-to-one, monotonic function with a differentiable inverse \\(f^{-1}\\). Then the density of \\(Y\\) is given by\n\\[p_Y(y) = p_X(f^{-1}(y)) \\left| \\frac{d}{dy} f^{-1}(y) \\right|\\]\nThe absolute derivative of the inverse transform measures how the scale of the transformed variable changes with respect to the underlying variable.\n\nIf you change in this way, you will change the prior on \\(\\sigma\\). You will not get the same log posterior density (lp__) as Model 1, since the prior on \\(\\sigma\\) is different.\nIn model 1: \\(\\sigma \\sim \\mathcal{N}(0, 5)\\)\nIn model 5: \\(\\log(\\sigma) \\sim \\mathcal{N}(0, 5)\\) or \\(\\sigma \\sim \\mathcal{LogN}(0, 5)\\)\nMy own opinion is that it is not recommended to transform the parameter locally inside the model block, since (1) it is not that transparent unless you really know what you are doing and (2) it will not be saved in the output.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Method 1: prior on log(sigma) --&gt; lead to a different prior on sigma\n  target += normal_lpdf(log(sigma) | 0, 5);\n  target += lognormal_lpdf(sigma | 0, 5); // equivalently\n\n  // Method 2: prior on local variable sigma_log with Jacobian adjustment\n  // real sigma_log = log(sigma);\n  // target += normal_lpdf(sigma_log | 0, 5);\n\n  // Priors\n  target += normal_lpdf(mu | 0, 10);\n\n  // Likelihood\n  target += normal_lpdf(y | mu, sigma);\n}\n\nmd_norm_transform_local &lt;- stan_model(file = \"./Models/normal_transform_local.stan\")\nfit_norm_transform_local &lt;- sampling(md_norm_transform_local, data_norm,\n  iter = 2000, chains = 1\n)\nprint(fit_norm_transform_local, pars = c(\"mu\", \"sigma\", \"lp__\"))\n\n\n\n\nAs we can see, the posterior parameter estimates for \\(\\mu\\) and \\(\\sigma\\) are similar across all three models. However, the log posterior density (lp__) differs between Model 1 and Model 2. This is because Model 1 includes the proper constraint on \\(\\sigma\\), while Model 2 does not. The log posterior density in Model 2 is biased due to the missing Jacobian adjustment. Model 3 addresses this issue by including a Jacobian adjustment. In general, if you are interested in parameter inference, it may be not a major concern in this case, but missing Jacobian adjustments can cause serious problems for model comparison (e.g., WAIC, LOO, and Bayes factors).\nNote that this example is only for illustration and help you understand the concept of Jacobian adjustment and how Stan handles changes of variables. In practice, you should always use the proper constraint on the parameter and let Stan handle the Jacobian adjustment automatically, which is both more efficient and more reliable.\nRelated links\n\n(Best) A coin toss example with Jacobian transformation\nThe Jacobian transformation\nChanges of variables\nTransforms\nLaplace method and Jacobian of parameter transformation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Data Science Toolkit",
    "section": "",
    "text": "Data Science Toolkit aims to provide comprehensive, practical resources for learners and practitioners. I believe that the right tools and knowledge should be accessible to everyone interested in learning to become a data scientist."
  },
  {
    "objectID": "about.html#our-mission",
    "href": "about.html#our-mission",
    "title": "About Data Science Toolkit",
    "section": "",
    "text": "Data Science Toolkit aims to provide comprehensive, practical resources for learners and practitioners. I believe that the right tools and knowledge should be accessible to everyone interested in learning to become a data scientist."
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "",
    "text": "Why I Switched from Kitty and Vifm to Ghostty and Yazi? After years of experimenting with different terminal emulators and file managers, I’ve finally found my favorite combination: Ghostty and Yazi. These have recently gained a lot of popularity in the community. Briefly put, Ghostty is a modern terminal emulator that is fast, feature-rich, and native. Yazi is a blazing-fast terminal file manager. Together, they have transformed my daily workflow. In this blog, I’ll show you how to set them up on your own computer, so you can supercharge your terminal experience just like I did.\nNote: here I assume the default fish shell is used. You can install it in this blog."
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-ghostty",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-ghostty",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "2.1 brew install ghostty",
    "text": "2.1 brew install ghostty\nbrew install --cask ghostty"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-ghostty-as-default-terminal-emulator-on-mac",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-ghostty-as-default-terminal-emulator-on-mac",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "2.2 set ghostty as default terminal emulator on mac",
    "text": "2.2 set ghostty as default terminal emulator on mac\n\n\nInstall the “RCDefaultApp.prefPane” plugin\n\n\nThis plugin is used to set the default app for opening terminal in your system.\ngit clone https://github.com/JakeJing/fishconfig.git\nsudo mv fishconfig/kitty/RCDefaultApp.prefPane /Library/PreferencePanes/\n\n\nSet ghostty as the “default app” for opening terminal\n\n\nGo to system preferences -&gt; default Apps -&gt; click the “default Apps” -&gt; URLS -&gt; x-man-page -&gt; set the default application as “ghostty”.\n\n\n\nSet ghostty as default app for opening terminal\n\n\n\n\nAdd keyboard shortcut (shift-cmd-1) to open a new Ghostty window here\n\n\nYou can set a keyboard shortcut to open Ghostty here. However, the default option only works when your cursor is on a folder. To enable it when your cursor is on a file, you can create a new service using the Automator application. To do this, go to Automator -&gt; Quick Action, and follow the steps in the image below.\n\n\n\nAutomator Quick Action\n\n\nAfter that, you can go to System Preferences -&gt; Keyboard -&gt; Shortcuts -&gt; Services -&gt; open-ghostty-here to add it (shift-cmd-1).\n\n\n\n\nAdd keyboard shortcut to open Ghostty here\n\n\nThis will automatically open a new Ghostty window here when you press shift-cmd-1. If there is an ongoing Ghostty window, it will open a new tab instead. So far there is no easy way to always open a new window (rather than a new tab or a new process) on mac, as far as I know.\n# check the services\nls ~/Library/Services/\n\n\nadd configuration file for ghostty\n\n\nwget https://raw.githubusercontent.com/JakeJing/dotfiles/refs/heads/main/.config/ghostty/config -P ~/.config/ghostty/"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-yazi-and-its-dependencies",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#brew-install-yazi-and-its-dependencies",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "3.1 brew install yazi and its dependencies",
    "text": "3.1 brew install yazi and its dependencies\nbrew update\nbrew install yazi ffmpeg sevenzip jq poppler fd ripgrep fzf zoxide resvg imagemagick font-symbols-only-nerd-font"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-alias-in-fish-config",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#set-alias-in-fish-config",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "3.2 set alias in fish config",
    "text": "3.2 set alias in fish config\n# yazi\nalias yz yazi\nalias y yazi\nalias a yazi"
  },
  {
    "objectID": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#add-configuration-file-and-keymap-for-yazi",
    "href": "commandline/ghostty-yazi-terminal/ghostty-yazi.html#add-configuration-file-and-keymap-for-yazi",
    "title": "Transform Your Terminal with Ghostty and Yazi",
    "section": "3.3 add configuration file and keymap for yazi",
    "text": "3.3 add configuration file and keymap for yazi\n# clone my dotfiles\ngit clone https://github.com/JakeJing/dotfiles.git\n# move yazi config\nmv dotfiles/.config/yazi -P ~/.config/yazi/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Data Science Toolkit",
    "section": "",
    "text": "Curated collections of programming languages for specific data science tasks:\n\nR Ecosystem: Tidyverse, and statistical packages\nStan: Bayesian analysis, Probabilistic programming\nData Visualization: ggplot2, Plotly\n\n\n\n\nStep-by-step guides and hands-on examples:\n\nGhostty/Kitty\nYazi/Vifm\nVisidata\n\n\n\n\nAdditional learning materials:\n\nRecommended books and courses\nDatasets for practice\nCommunity resources"
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Welcome to Data Science Toolkit",
    "section": "",
    "text": "Curated collections of programming languages for specific data science tasks:\n\nR Ecosystem: Tidyverse, and statistical packages\nStan: Bayesian analysis, Probabilistic programming\nData Visualization: ggplot2, Plotly\n\n\n\n\nStep-by-step guides and hands-on examples:\n\nGhostty/Kitty\nYazi/Vifm\nVisidata\n\n\n\n\nAdditional learning materials:\n\nRecommended books and courses\nDatasets for practice\nCommunity resources"
  },
  {
    "objectID": "commandline/coverpage.html",
    "href": "commandline/coverpage.html",
    "title": "Command Line Tools",
    "section": "",
    "text": "Transform Your Terminal with Ghostty and Yazi - Ready your terminal emulator and file manager"
  },
  {
    "objectID": "programming/coverpage.html",
    "href": "programming/coverpage.html",
    "title": "Programming",
    "section": "",
    "text": "Understanding Jacobian Adjustments for Constrained Parameters in Stan - A simple example to illustrate how Jacobian adjustment works in Stan"
  }
]